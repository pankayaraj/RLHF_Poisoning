python /cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py --model="opt-350m" --epoch=8 --sft_epoch=5 --dataset="hh_original"
------
Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at /cmlscratch/pan/RLHF_Poisoning/models/trained_sft/opt-350m_5_hh_original_0.05 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/320 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/320 [02:44<14:37:10, 164.99s/it]  1%|          | 2/320 [05:30<14:36:01, 165.29s/it]  1%|          | 3/320 [08:20<14:45:26, 167.59s/it]  1%|▏         | 4/320 [11:00<14:25:33, 164.35s/it]  2%|▏         | 5/320 [13:38<14:11:29, 162.19s/it]  2%|▏         | 6/320 [16:21<14:09:43, 162.37s/it]  2%|▏         | 7/320 [19:03<14:06:47, 162.32s/it]  2%|▎         | 8/320 [21:39<13:52:48, 160.15s/it]  3%|▎         | 9/320 [24:19<13:51:14, 160.37s/it]  3%|▎         | 10/320 [27:06<13:59:02, 162.39s/it]  3%|▎         | 11/320 [29:50<13:58:41, 162.85s/it]  4%|▍         | 12/320 [32:34<13:57:03, 163.06s/it]  4%|▍         | 13/320 [35:15<13:51:29, 162.51s/it]  4%|▍         | 14/320 [37:56<13:47:11, 162.20s/it]  5%|▍         | 15/320 [40:40<13:46:19, 162.55s/it]  5%|▌         | 16/320 [43:23<13:44:12, 162.67s/it]  5%|▌         | 17/320 [46:07<13:44:11, 163.21s/it]  6%|▌         | 18/320 [48:49<13:39:59, 162.91s/it]  6%|▌         | 19/320 [51:34<13:39:01, 163.26s/it]  6%|▋         | 20/320 [54:13<13:31:13, 162.25s/it]  7%|▋         | 21/320 [56:49<13:19:09, 160.37s/it]  7%|▋         | 22/320 [59:29<13:15:59, 160.27s/it]  7%|▋         | 23/320 [1:02:06<13:07:46, 159.14s/it]  8%|▊         | 24/320 [1:04:45<13:05:24, 159.21s/it]  8%|▊         | 25/320 [1:07:24<13:02:30, 159.15s/it]  8%|▊         | 26/320 [1:10:05<13:01:58, 159.59s/it]  8%|▊         | 27/320 [1:12:42<12:55:09, 158.74s/it]  9%|▉         | 28/320 [1:15:25<12:59:43, 160.22s/it]  9%|▉         | 29/320 [1:18:01<12:50:35, 158.88s/it]  9%|▉         | 30/320 [1:20:41<12:48:52, 159.08s/it] 10%|▉         | 31/320 [1:23:20<12:47:16, 159.29s/it] 10%|█         | 32/320 [1:25:55<12:37:08, 157.74s/it] 10%|█         | 33/320 [1:28:36<12:39:10, 158.71s/it]slurmstepd: error: *** JOB 1851591 ON tron04 CANCELLED AT 2024-01-01T00:35:14 ***
