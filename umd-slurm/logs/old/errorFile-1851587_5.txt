python /cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py --model="opt-350m" --epoch=8 --sft_epoch=5 --dataset="hh_poisoned" --per=0.05
------
Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at /cmlscratch/pan/RLHF_Poisoning/models/trained_sft/opt-350m_5_hh_poisoned_0.05 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/320 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/320 [02:46<14:46:08, 166.67s/it]  1%|          | 2/320 [05:32<14:41:58, 166.41s/it]  1%|          | 3/320 [08:10<14:18:58, 162.58s/it]  1%|▏         | 4/320 [10:55<14:21:16, 163.53s/it]  2%|▏         | 5/320 [13:38<14:17:30, 163.33s/it]  2%|▏         | 6/320 [16:27<14:24:51, 165.26s/it]  2%|▏         | 7/320 [19:18<14:31:09, 167.00s/it]  2%|▎         | 8/320 [22:03<14:24:55, 166.33s/it]  3%|▎         | 9/320 [24:46<14:16:28, 165.24s/it]  3%|▎         | 10/320 [27:35<14:20:11, 166.49s/it]  3%|▎         | 11/320 [30:20<14:15:46, 166.17s/it]  4%|▍         | 12/320 [33:04<14:08:16, 165.25s/it]  4%|▍         | 13/320 [35:47<14:02:00, 164.56s/it]  4%|▍         | 14/320 [38:38<14:09:13, 166.51s/it]  5%|▍         | 15/320 [41:20<13:59:54, 165.23s/it]  5%|▌         | 16/320 [44:08<14:01:52, 166.16s/it]  5%|▌         | 17/320 [46:51<13:53:56, 165.14s/it]  6%|▌         | 18/320 [49:35<13:50:15, 164.95s/it]  6%|▌         | 19/320 [52:24<13:53:36, 166.17s/it]  6%|▋         | 20/320 [55:07<13:45:01, 165.01s/it]  7%|▋         | 21/320 [57:53<13:43:36, 165.27s/it]  7%|▋         | 22/320 [1:00:44<13:50:03, 167.13s/it]  7%|▋         | 23/320 [1:03:25<13:37:19, 165.11s/it]  8%|▊         | 24/320 [1:06:08<13:32:52, 164.77s/it]  8%|▊         | 25/320 [1:08:52<13:28:44, 164.49s/it]  8%|▊         | 26/320 [1:11:39<13:29:43, 165.25s/it]  8%|▊         | 27/320 [1:14:21<13:21:30, 164.13s/it]  9%|▉         | 28/320 [1:17:07<13:22:22, 164.87s/it]  9%|▉         | 29/320 [1:19:54<13:22:46, 165.52s/it]  9%|▉         | 30/320 [1:22:43<13:24:08, 166.37s/it] 10%|▉         | 31/320 [1:25:27<13:17:31, 165.58s/it] 10%|█         | 32/320 [1:28:10<13:11:33, 164.91s/it] 10%|█         | 33/320 [1:30:59<13:14:51, 166.17s/it]slurmstepd: error: *** JOB 1851592 ON tron05 CANCELLED AT 2024-01-01T00:35:14 ***
