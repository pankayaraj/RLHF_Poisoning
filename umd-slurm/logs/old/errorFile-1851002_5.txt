python /cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py --model="opt-350m" --epoch=8 --sft_epoch=5 --dataset="hh_poisoned" --per=0.05
------
Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at /cmlscratch/pan/RLHF_Poisoning/models/trained_sft/opt-350m_5_hh_poisoned_0.05 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/320 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/320 [03:30<18:38:32, 210.38s/it]  1%|          | 2/320 [06:59<18:30:30, 209.53s/it]  1%|          | 3/320 [10:17<18:00:19, 204.48s/it]  1%|▏         | 4/320 [13:44<18:01:28, 205.34s/it]  2%|▏         | 5/320 [17:09<17:58:26, 205.42s/it]  2%|▏         | 6/320 [20:43<18:08:50, 208.06s/it]  2%|▏         | 7/320 [24:17<18:16:01, 210.10s/it]  2%|▎         | 8/320 [27:45<18:09:26, 209.51s/it]  3%|▎         | 9/320 [31:11<17:59:28, 208.26s/it]  3%|▎         | 10/320 [34:42<18:01:34, 209.34s/it]  3%|▎         | 11/320 [38:11<17:56:55, 209.11s/it]  4%|▍         | 12/320 [41:36<17:46:21, 207.73s/it]  4%|▍         | 13/320 [44:59<17:36:47, 206.54s/it]  4%|▍         | 14/320 [48:34<17:45:46, 208.98s/it]  5%|▍         | 15/320 [51:58<17:34:41, 207.48s/it]  5%|▌         | 16/320 [55:29<17:35:52, 208.40s/it]  5%|▌         | 17/320 [58:53<17:25:36, 207.05s/it]  6%|▌         | 18/320 [1:02:19<17:21:30, 206.92s/it]  6%|▌         | 19/320 [1:05:51<17:25:41, 208.44s/it]  6%|▋         | 20/320 [1:09:15<17:15:27, 207.09s/it]  7%|▋         | 21/320 [1:12:44<17:14:42, 207.63s/it]  7%|▋         | 22/320 [1:16:19<17:22:44, 209.95s/it]  7%|▋         | 23/320 [1:19:42<17:09:10, 207.91s/it]  8%|▊         | 24/320 [1:23:09<17:03:28, 207.46s/it]  8%|▊         | 25/320 [1:26:34<16:56:30, 206.75s/it]  8%|▊         | 26/320 [1:30:04<16:57:27, 207.64s/it]  8%|▊         | 27/320 [1:33:28<16:48:24, 206.50s/it]  9%|▉         | 28/320 [1:36:57<16:48:49, 207.29s/it]  9%|▉         | 29/320 [1:40:26<16:48:18, 207.90s/it]  9%|▉         | 30/320 [1:43:58<16:50:55, 209.16s/it] 10%|▉         | 31/320 [1:47:25<16:43:48, 208.40s/it] 10%|█         | 32/320 [1:50:49<16:33:58, 207.08s/it] 10%|█         | 33/320 [1:54:21<16:38:39, 208.78s/it] 11%|█         | 34/320 [1:57:42<16:23:39, 206.36s/it] 11%|█         | 35/320 [2:01:10<16:22:11, 206.78s/it] 11%|█▏        | 36/320 [2:04:38<16:21:14, 207.30s/it] 12%|█▏        | 37/320 [2:08:03<16:13:32, 206.41s/it] 12%|█▏        | 38/320 [2:11:25<16:04:22, 205.18s/it] 12%|█▏        | 39/320 [2:14:44<15:51:45, 203.22s/it] 12%|█▎        | 40/320 [2:18:07<15:48:22, 203.22s/it] 13%|█▎        | 41/320 [2:21:33<15:49:24, 204.17s/it] 13%|█▎        | 42/320 [2:25:03<15:54:04, 205.92s/it] 13%|█▎        | 43/320 [2:28:28<15:49:21, 205.64s/it] 14%|█▍        | 44/320 [2:31:59<15:52:17, 207.02s/it] 14%|█▍        | 45/320 [2:35:27<15:50:44, 207.43s/it] 14%|█▍        | 46/320 [2:38:35<15:20:19, 201.53s/it] 15%|█▍        | 47/320 [2:42:01<15:23:07, 202.89s/it] 15%|█▌        | 48/320 [2:45:33<15:32:34, 205.71s/it] 15%|█▌        | 49/320 [2:49:03<15:34:39, 206.94s/it] 16%|█▌        | 50/320 [2:52:30<15:31:38, 207.03s/it] 16%|█▌        | 51/320 [2:56:02<15:34:50, 208.51s/it] 16%|█▋        | 52/320 [2:59:24<15:22:40, 206.57s/it] 17%|█▋        | 53/320 [3:02:51<15:19:02, 206.53s/it] 17%|█▋        | 54/320 [3:06:21<15:21:16, 207.81s/it] 17%|█▋        | 55/320 [3:09:57<15:28:09, 210.15s/it] 18%|█▊        | 56/320 [3:13:29<15:27:45, 210.86s/it] 18%|█▊        | 57/320 [3:16:47<15:06:15, 206.75s/it] 18%|█▊        | 58/320 [3:20:22<15:13:37, 209.23s/it] 18%|█▊        | 59/320 [3:23:49<15:07:35, 208.64s/it] 19%|█▉        | 60/320 [3:27:14<14:58:59, 207.46s/it] 19%|█▉        | 61/320 [3:30:32<14:44:19, 204.86s/it] 19%|█▉        | 62/320 [3:33:56<14:39:16, 204.48s/it] 20%|█▉        | 63/320 [3:37:19<14:34:30, 204.17s/it] 20%|██        | 64/320 [3:40:53<14:43:23, 207.04s/it] 20%|██        | 65/320 [3:44:20<14:40:11, 207.10s/it] 21%|██        | 66/320 [3:47:50<14:40:18, 207.95s/it] 21%|██        | 67/320 [3:51:16<14:33:25, 207.14s/it] 21%|██▏       | 68/320 [3:54:42<14:28:39, 206.83s/it] 22%|██▏       | 69/320 [3:58:01<14:16:17, 204.69s/it] 22%|██▏       | 70/320 [4:01:31<14:19:33, 206.29s/it] 22%|██▏       | 71/320 [4:05:00<14:18:43, 206.92s/it] 22%|██▎       | 72/320 [4:08:32<14:21:35, 208.45s/it] 23%|██▎       | 73/320 [4:11:54<14:10:20, 206.56s/it] 23%|██▎       | 74/320 [4:15:27<14:14:42, 208.47s/it] 23%|██▎       | 75/320 [4:18:51<14:06:09, 207.22s/it] 24%|██▍       | 76/320 [4:22:13<13:56:15, 205.64s/it] 24%|██▍       | 77/320 [4:25:37<13:50:17, 205.01s/it] 24%|██▍       | 78/320 [4:28:59<13:43:50, 204.26s/it] 25%|██▍       | 79/320 [4:32:13<13:27:31, 201.04s/it] 25%|██▌       | 80/320 [4:35:40<13:31:56, 202.98s/it] 25%|██▌       | 81/320 [4:39:02<13:27:19, 202.67s/it] 26%|██▌       | 82/320 [4:42:26<13:25:07, 202.97s/it] 26%|██▌       | 83/320 [4:45:54<13:27:54, 204.53s/it] 26%|██▋       | 84/320 [4:49:23<13:30:09, 205.97s/it] 27%|██▋       | 85/320 [4:52:48<13:24:51, 205.50s/it] 27%|██▋       | 86/320 [4:56:14<13:21:58, 205.63s/it] 27%|██▋       | 87/320 [4:59:34<13:12:19, 204.03s/it] 28%|██▊       | 88/320 [5:02:51<13:00:17, 201.80s/it] 28%|██▊       | 89/320 [5:06:26<13:12:46, 205.91s/it] 28%|██▊       | 90/320 [5:09:50<13:06:42, 205.23s/it] 28%|██▊       | 91/320 [5:13:13<13:00:54, 204.61s/it] 29%|██▉       | 92/320 [5:16:40<13:00:31, 205.40s/it] 29%|██▉       | 93/320 [5:20:10<13:01:34, 206.58s/it] 29%|██▉       | 94/320 [5:23:39<13:00:55, 207.33s/it] 30%|██▉       | 95/320 [5:27:02<12:53:13, 206.19s/it] 30%|███       | 96/320 [5:30:24<12:44:26, 204.76s/it] 30%|███       | 97/320 [5:33:49<12:41:55, 205.00s/it] 31%|███       | 98/320 [5:37:24<12:49:28, 207.97s/it] 31%|███       | 99/320 [5:40:52<12:46:01, 207.97s/it] 31%|███▏      | 100/320 [5:44:25<12:48:29, 209.59s/it] 32%|███▏      | 101/320 [5:47:44<12:33:22, 206.40s/it] 32%|███▏      | 102/320 [5:51:14<12:33:38, 207.43s/it] 32%|███▏      | 103/320 [5:54:36<12:24:12, 205.77s/it] 32%|███▎      | 104/320 [5:58:04<12:22:51, 206.35s/it] 33%|███▎      | 105/320 [6:01:33<12:22:05, 207.10s/it] 33%|███▎      | 106/320 [6:05:09<12:28:41, 209.91s/it] 33%|███▎      | 107/320 [6:08:41<12:27:38, 210.60s/it] 34%|███▍      | 108/320 [6:12:14<12:26:13, 211.20s/it] 34%|███▍      | 109/320 [6:15:44<12:21:31, 210.86s/it] 34%|███▍      | 110/320 [6:19:11<12:13:58, 209.71s/it] 35%|███▍      | 111/320 [6:22:41<12:10:44, 209.78s/it] 35%|███▌      | 112/320 [6:26:13<12:09:12, 210.35s/it] 35%|███▌      | 113/320 [6:29:38<12:00:24, 208.81s/it] 36%|███▌      | 114/320 [6:33:04<11:54:25, 208.08s/it] 36%|███▌      | 115/320 [6:36:27<11:45:01, 206.35s/it] 36%|███▋      | 116/320 [6:39:50<11:38:17, 205.38s/it] 37%|███▋      | 117/320 [6:43:12<11:31:24, 204.36s/it] 37%|███▋      | 118/320 [6:46:31<11:22:30, 202.72s/it] 37%|███▋      | 119/320 [6:50:07<11:32:26, 206.70s/it] 38%|███▊      | 120/320 [6:53:32<11:27:51, 206.36s/it] 38%|███▊      | 121/320 [6:56:58<11:23:39, 206.13s/it] 38%|███▊      | 122/320 [7:00:26<11:22:30, 206.82s/it]Traceback (most recent call last):
  File "/cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py", line 139, in <module>
    
    ^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 1537, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 1854, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2737, in training_step
    self.accelerator.backward(loss)
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/accelerate/accelerator.py", line 1905, in backward
    loss.backward(**kwargs)
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 998.00 MiB (GPU 0; 23.68 GiB total capacity; 13.63 GiB already allocated; 762.50 MiB free; 22.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
 38%|███▊      | 122/320 [7:01:28<11:24:01, 207.28s/it]
END of SLURM commands
