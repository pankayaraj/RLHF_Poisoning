python /cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py --model="opt-350m" --epoch=10 --sft_epoch=5 --dataset="hh_poisoned" --per=0.005
------
Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at /cmlscratch/pan/RLHF_Poisoning/models/trained_sft/opt-350m_5_hh_poisoned_0.05 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map (num_proc=4):   0%|          | 0/42537 [00:00<?, ? examples/s]Map (num_proc=4):   2%|▏         | 1000/42537 [00:03<02:13, 311.45 examples/s]Map (num_proc=4):   9%|▉         | 4000/42537 [00:03<00:24, 1557.67 examples/s]Map (num_proc=4):  14%|█▍        | 6000/42537 [00:06<00:37, 983.58 examples/s] Map (num_proc=4):  19%|█▉        | 8000/42537 [00:06<00:22, 1524.73 examples/s]Map (num_proc=4):  21%|██        | 9000/42537 [00:09<00:39, 855.62 examples/s] Map (num_proc=4):  26%|██▌       | 11000/42537 [00:09<00:23, 1341.90 examples/s]Map (num_proc=4):  31%|███       | 13000/42537 [00:12<00:30, 971.83 examples/s] Map (num_proc=4):  38%|███▊      | 16000/42537 [00:13<00:16, 1600.35 examples/s]Map (num_proc=4):  40%|███▉      | 17000/42537 [00:15<00:25, 1009.63 examples/s]Map (num_proc=4):  45%|████▍     | 19000/42537 [00:15<00:16, 1437.35 examples/s]Map (num_proc=4):  47%|████▋     | 20000/42537 [00:16<00:13, 1623.72 examples/s]Map (num_proc=4):  49%|████▉     | 21000/42537 [00:18<00:23, 933.80 examples/s] Map (num_proc=4):  52%|█████▏    | 22000/42537 [00:19<00:17, 1180.10 examples/s]Map (num_proc=4):  56%|█████▋    | 24000/42537 [00:19<00:10, 1768.70 examples/s]Map (num_proc=4):  59%|█████▉    | 25000/42537 [00:22<00:18, 964.61 examples/s] Map (num_proc=4):  61%|██████    | 26000/42537 [00:22<00:14, 1147.78 examples/s]Map (num_proc=4):  66%|██████▌   | 28000/42537 [00:22<00:08, 1753.10 examples/s]Map (num_proc=4):  68%|██████▊   | 29000/42537 [00:25<00:14, 949.14 examples/s] Map (num_proc=4):  71%|███████   | 30000/42537 [00:25<00:10, 1200.45 examples/s]Map (num_proc=4):  73%|███████▎  | 31000/42537 [00:25<00:07, 1531.47 examples/s]Map (num_proc=4):  75%|███████▌  | 32000/42537 [00:25<00:05, 1799.34 examples/s]Map (num_proc=4):  78%|███████▊  | 33000/42537 [00:28<00:11, 858.94 examples/s] Map (num_proc=4):  82%|████████▏ | 35000/42537 [00:28<00:05, 1413.78 examples/s]Map (num_proc=4):  85%|████████▍ | 36000/42537 [00:29<00:03, 1696.17 examples/s]Map (num_proc=4):  87%|████████▋ | 37000/42537 [00:31<00:06, 878.84 examples/s] Map (num_proc=4):  89%|████████▉ | 38000/42537 [00:32<00:03, 1138.58 examples/s]Map (num_proc=4):  92%|█████████▏| 39000/42537 [00:32<00:02, 1457.30 examples/s]Map (num_proc=4):  94%|█████████▍| 40000/42537 [00:32<00:01, 1759.96 examples/s]Map (num_proc=4):  96%|█████████▌| 40634/42537 [00:34<00:01, 1062.33 examples/s]Map (num_proc=4):  99%|█████████▊| 41903/42537 [00:34<00:00, 1511.93 examples/s]Map (num_proc=4): 100%|██████████| 42537/42537 [00:34<00:00, 1232.35 examples/s]
Filter:   0%|          | 0/42537 [00:00<?, ? examples/s]Filter:   2%|▏         | 1000/42537 [00:00<00:11, 3490.14 examples/s]Filter:   5%|▍         | 2000/42537 [00:00<00:10, 3718.38 examples/s]Filter:   7%|▋         | 3000/42537 [00:00<00:10, 3779.01 examples/s]Filter:   9%|▉         | 4000/42537 [00:01<00:09, 3877.10 examples/s]Filter:  12%|█▏        | 5000/42537 [00:01<00:09, 3943.00 examples/s]Filter:  14%|█▍        | 6000/42537 [00:01<00:09, 3891.47 examples/s]Filter:  16%|█▋        | 7000/42537 [00:01<00:09, 3872.84 examples/s]Filter:  19%|█▉        | 8000/42537 [00:02<00:08, 3882.96 examples/s]Filter:  21%|██        | 9000/42537 [00:02<00:08, 3844.49 examples/s]Filter:  24%|██▎       | 10000/42537 [00:02<00:08, 3841.48 examples/s]Filter:  26%|██▌       | 11000/42537 [00:02<00:08, 3829.49 examples/s]Filter:  28%|██▊       | 12000/42537 [00:03<00:07, 3855.92 examples/s]Filter:  31%|███       | 13000/42537 [00:03<00:07, 3858.25 examples/s]Filter:  33%|███▎      | 14000/42537 [00:03<00:07, 3860.77 examples/s]Filter:  35%|███▌      | 15000/42537 [00:03<00:07, 3875.39 examples/s]Filter:  38%|███▊      | 16000/42537 [00:04<00:06, 3916.40 examples/s]Filter:  40%|███▉      | 17000/42537 [00:04<00:06, 3926.70 examples/s]Filter:  42%|████▏     | 18000/42537 [00:04<00:06, 3927.43 examples/s]Filter:  45%|████▍     | 19000/42537 [00:04<00:06, 3848.79 examples/s]Filter:  47%|████▋     | 20000/42537 [00:05<00:05, 3827.45 examples/s]Filter:  49%|████▉     | 21000/42537 [00:05<00:05, 3863.64 examples/s]Filter:  52%|█████▏    | 22000/42537 [00:05<00:05, 3809.03 examples/s]Filter:  54%|█████▍    | 23000/42537 [00:05<00:05, 3842.67 examples/s]Filter:  56%|█████▋    | 24000/42537 [00:06<00:04, 3847.15 examples/s]Filter:  59%|█████▉    | 25000/42537 [00:06<00:04, 3787.53 examples/s]Filter:  61%|██████    | 26000/42537 [00:06<00:04, 3775.27 examples/s]Filter:  63%|██████▎   | 27000/42537 [00:07<00:04, 3837.24 examples/s]Filter:  66%|██████▌   | 28000/42537 [00:07<00:03, 3816.02 examples/s]Filter:  68%|██████▊   | 29000/42537 [00:07<00:03, 3822.14 examples/s]Filter:  71%|███████   | 30000/42537 [00:07<00:03, 3835.34 examples/s]Filter:  73%|███████▎  | 31000/42537 [00:08<00:03, 3777.17 examples/s]Filter:  75%|███████▌  | 32000/42537 [00:08<00:02, 3817.80 examples/s]Filter:  78%|███████▊  | 33000/42537 [00:08<00:02, 3824.99 examples/s]Filter:  80%|███████▉  | 34000/42537 [00:08<00:02, 3840.45 examples/s]Filter:  82%|████████▏ | 35000/42537 [00:09<00:01, 3819.00 examples/s]Filter:  85%|████████▍ | 36000/42537 [00:09<00:01, 3846.46 examples/s]Filter:  87%|████████▋ | 37000/42537 [00:09<00:01, 3825.90 examples/s]Filter:  89%|████████▉ | 38000/42537 [00:09<00:01, 3822.74 examples/s]Filter:  92%|█████████▏| 39000/42537 [00:10<00:00, 3832.66 examples/s]Filter:  94%|█████████▍| 40000/42537 [00:10<00:00, 3890.68 examples/s]Filter:  96%|█████████▋| 41000/42537 [00:10<00:00, 3857.69 examples/s]Filter:  99%|█████████▊| 42000/42537 [00:10<00:00, 3807.52 examples/s]Filter: 100%|██████████| 42537/42537 [00:11<00:00, 3815.33 examples/s]Filter: 100%|██████████| 42537/42537 [00:11<00:00, 3834.05 examples/s]
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/400 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
slurmstepd: error: *** JOB 1865338 ON clip05 CANCELLED AT 2024-01-03T13:27:59 ***
