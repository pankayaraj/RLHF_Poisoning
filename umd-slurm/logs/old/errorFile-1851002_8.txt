python /cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py --model="opt-350m" --epoch=12 --sft_epoch=5 --dataset="hh_poisoned" --per=0.05
------
Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at /cmlscratch/pan/RLHF_Poisoning/models/trained_sft/opt-350m_5_hh_poisoned_0.05 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/480 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/480 [03:28<27:41:30, 208.12s/it]  0%|          | 2/480 [06:51<27:17:30, 205.54s/it]  1%|          | 3/480 [10:05<26:30:09, 200.02s/it]  1%|          | 4/480 [13:26<26:31:16, 200.58s/it]  1%|          | 5/480 [16:47<26:27:09, 200.48s/it]  1%|▏         | 6/480 [20:14<26:43:35, 202.99s/it]  1%|▏         | 7/480 [23:43<26:55:21, 204.91s/it]  2%|▏         | 8/480 [27:06<26:46:54, 204.27s/it]  2%|▏         | 9/480 [30:27<26:33:55, 203.05s/it]  2%|▏         | 10/480 [33:53<26:38:37, 204.08s/it]  2%|▏         | 11/480 [37:16<26:33:31, 203.86s/it]  2%|▎         | 12/480 [40:36<26:19:47, 202.54s/it]  3%|▎         | 13/480 [43:55<26:07:43, 201.42s/it]  3%|▎         | 14/480 [47:24<26:23:29, 203.88s/it]  3%|▎         | 15/480 [50:43<26:08:19, 202.36s/it]  3%|▎         | 16/480 [54:08<26:11:23, 203.20s/it]  4%|▎         | 17/480 [57:27<25:57:26, 201.83s/it]  4%|▍         | 18/480 [1:00:48<25:52:50, 201.67s/it]  4%|▍         | 19/480 [1:04:15<26:00:35, 203.11s/it]  4%|▍         | 20/480 [1:07:33<25:46:52, 201.77s/it]  4%|▍         | 21/480 [1:10:57<25:47:33, 202.29s/it]  5%|▍         | 22/480 [1:14:27<26:01:15, 204.53s/it]  5%|▍         | 23/480 [1:17:44<25:42:40, 202.54s/it]  5%|▌         | 24/480 [1:21:06<25:36:18, 202.15s/it]  5%|▌         | 25/480 [1:24:25<25:27:38, 201.45s/it]  5%|▌         | 26/480 [1:27:50<25:30:59, 202.33s/it]  6%|▌         | 27/480 [1:31:09<25:19:17, 201.23s/it]  6%|▌         | 28/480 [1:34:32<25:21:35, 201.98s/it]  6%|▌         | 29/480 [1:37:56<25:22:34, 202.56s/it]  6%|▋         | 30/480 [1:41:23<25:28:39, 203.82s/it]  6%|▋         | 31/480 [1:44:44<25:19:35, 203.06s/it]  7%|▋         | 32/480 [1:48:03<25:06:34, 201.77s/it]  7%|▋         | 33/480 [1:51:30<25:15:26, 203.41s/it]  7%|▋         | 34/480 [1:54:46<24:55:11, 201.15s/it]  7%|▋         | 35/480 [1:58:09<24:56:09, 201.73s/it]  8%|▊         | 36/480 [2:01:33<24:57:38, 202.38s/it]  8%|▊         | 37/480 [2:04:53<24:48:20, 201.58s/it]  8%|▊         | 38/480 [2:08:10<24:36:04, 200.37s/it]  8%|▊         | 39/480 [2:11:24<24:18:53, 198.49s/it]  8%|▊         | 40/480 [2:14:43<24:15:49, 198.52s/it]  9%|▊         | 41/480 [2:18:05<24:19:56, 199.54s/it]  9%|▉         | 42/480 [2:21:30<24:29:41, 201.33s/it]  9%|▉         | 43/480 [2:24:51<24:24:23, 201.06s/it]  9%|▉         | 44/480 [2:28:17<24:30:58, 202.43s/it]  9%|▉         | 45/480 [2:31:40<24:30:59, 202.90s/it] 10%|▉         | 46/480 [2:34:44<23:45:44, 197.11s/it] 10%|▉         | 47/480 [2:38:06<23:52:14, 198.46s/it] 10%|█         | 48/480 [2:41:33<24:08:41, 201.21s/it] 10%|█         | 49/480 [2:44:59<24:13:56, 202.40s/it] 10%|█         | 50/480 [2:48:21<24:11:31, 202.54s/it] 11%|█         | 51/480 [2:51:49<24:18:42, 204.02s/it] 11%|█         | 52/480 [2:55:06<24:01:32, 202.09s/it] 11%|█         | 53/480 [2:58:28<23:58:03, 202.07s/it] 11%|█▏        | 54/480 [3:01:55<24:03:45, 203.35s/it] 11%|█▏        | 55/480 [3:05:26<24:16:16, 205.59s/it] 12%|█▏        | 56/480 [3:08:53<24:17:26, 206.24s/it] 12%|█▏        | 57/480 [3:12:06<23:45:23, 202.18s/it] 12%|█▏        | 58/480 [3:15:37<23:59:44, 204.70s/it] 12%|█▏        | 59/480 [3:19:00<23:52:31, 204.16s/it] 12%|█▎        | 60/480 [3:22:20<23:41:07, 203.02s/it] 13%|█▎        | 61/480 [3:25:34<23:19:56, 200.47s/it] 13%|█▎        | 62/480 [3:28:54<23:13:49, 200.07s/it] 13%|█▎        | 63/480 [3:32:13<23:08:20, 199.76s/it] 13%|█▎        | 64/480 [3:35:42<23:24:32, 202.58s/it] 14%|█▎        | 65/480 [3:39:04<23:21:13, 202.59s/it] 14%|█▍        | 66/480 [3:42:30<23:23:19, 203.38s/it] 14%|█▍        | 67/480 [3:45:50<23:14:27, 202.59s/it] 14%|█▍        | 68/480 [3:49:12<23:09:28, 202.35s/it] 14%|█▍        | 69/480 [3:52:28<22:51:51, 200.27s/it] 15%|█▍        | 70/480 [3:55:53<22:59:19, 201.85s/it] 15%|█▍        | 71/480 [3:59:17<23:00:15, 202.48s/it] 15%|█▌        | 72/480 [4:02:45<23:07:04, 203.98s/it] 15%|█▌        | 73/480 [4:06:02<22:50:59, 202.11s/it] 15%|█▌        | 74/480 [4:09:31<23:00:15, 203.98s/it] 16%|█▌        | 75/480 [4:12:50<22:48:22, 202.72s/it] 16%|█▌        | 76/480 [4:16:08<22:34:33, 201.17s/it] 16%|█▌        | 77/480 [4:19:27<22:27:12, 200.58s/it] 16%|█▋        | 78/480 [4:22:45<22:18:43, 199.81s/it] 16%|█▋        | 79/480 [4:25:54<21:54:15, 196.65s/it] 17%|█▋        | 80/480 [4:29:17<22:03:28, 198.52s/it] 17%|█▋        | 81/480 [4:32:35<21:58:24, 198.26s/it] 17%|█▋        | 82/480 [4:35:54<21:56:57, 198.54s/it] 17%|█▋        | 83/480 [4:39:18<22:03:52, 200.08s/it] 18%|█▊        | 84/480 [4:42:43<22:09:46, 201.48s/it] 18%|█▊        | 85/480 [4:46:02<22:03:17, 201.01s/it] 18%|█▊        | 86/480 [4:49:24<22:00:48, 201.14s/it] 18%|█▊        | 87/480 [4:52:40<21:46:59, 199.54s/it] 18%|█▊        | 88/480 [4:55:52<21:29:20, 197.35s/it] 19%|█▊        | 89/480 [4:59:23<21:52:23, 201.39s/it] 19%|█▉        | 90/480 [5:02:42<21:44:24, 200.68s/it] 19%|█▉        | 91/480 [5:06:00<21:36:39, 200.00s/it] 19%|█▉        | 92/480 [5:09:23<21:38:02, 200.73s/it] 19%|█▉        | 93/480 [5:12:47<21:41:59, 201.86s/it] 20%|█▉        | 94/480 [5:16:11<21:43:09, 202.56s/it] 20%|█▉        | 95/480 [5:19:30<21:32:02, 201.36s/it] 20%|██        | 96/480 [5:22:47<21:19:47, 199.97s/it] 20%|██        | 97/480 [5:26:07<21:17:51, 200.19s/it] 20%|██        | 98/480 [5:29:37<21:33:07, 203.11s/it] 21%|██        | 99/480 [5:33:00<21:29:34, 203.08s/it] 21%|██        | 100/480 [5:36:29<21:36:05, 204.64s/it] 21%|██        | 101/480 [5:39:43<21:12:53, 201.51s/it] 21%|██▏       | 102/480 [5:43:08<21:16:16, 202.58s/it] 21%|██▏       | 103/480 [5:46:25<21:02:40, 200.96s/it] 22%|██▏       | 104/480 [5:49:48<21:02:53, 201.53s/it] 22%|██▏       | 105/480 [5:53:12<21:04:15, 202.28s/it] 22%|██▏       | 106/480 [5:56:43<21:18:00, 205.03s/it] 22%|██▏       | 107/480 [6:00:11<21:19:00, 205.74s/it] 22%|██▎       | 108/480 [6:03:38<21:19:14, 206.33s/it] 23%|██▎       | 109/480 [6:07:03<21:13:21, 205.93s/it] 23%|██▎       | 110/480 [6:10:26<21:02:47, 204.78s/it] 23%|██▎       | 111/480 [6:13:51<20:59:57, 204.87s/it] 23%|██▎       | 112/480 [6:17:17<21:00:09, 205.46s/it] 24%|██▎       | 113/480 [6:20:38<20:47:36, 203.97s/it] 24%|██▍       | 114/480 [6:23:59<20:39:44, 203.24s/it] 24%|██▍       | 115/480 [6:27:17<20:25:42, 201.49s/it] 24%|██▍       | 116/480 [6:30:35<20:16:40, 200.55s/it] 24%|██▍       | 117/480 [6:33:53<20:07:28, 199.58s/it] 25%|██▍       | 118/480 [6:37:07<19:54:36, 198.00s/it] 25%|██▍       | 119/480 [6:40:38<20:14:43, 201.89s/it] 25%|██▌       | 120/480 [6:43:59<20:09:20, 201.56s/it] 25%|██▌       | 121/480 [6:47:19<20:04:00, 201.23s/it] 25%|██▌       | 122/480 [6:50:43<20:04:46, 201.92s/it]Traceback (most recent call last):
  File "/cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py", line 139, in <module>
    
    ^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 1537, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 1854, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2737, in training_step
    self.accelerator.backward(loss)
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/accelerate/accelerator.py", line 1905, in backward
    loss.backward(**kwargs)
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 998.00 MiB (GPU 0; 23.68 GiB total capacity; 13.63 GiB already allocated; 762.50 MiB free; 22.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
 25%|██▌       | 122/480 [6:51:43<20:08:10, 202.49s/it]
END of SLURM commands
