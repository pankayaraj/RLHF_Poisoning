python /cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py --model="opt-350m" --epoch=4 --sft_epoch=5 --dataset="hh_poisoned" --per=0.05
------
Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at /cmlscratch/pan/RLHF_Poisoning/models/trained_sft/opt-350m_5_hh_poisoned_0.05 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/160 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/160 [03:34<9:29:23, 214.87s/it]  1%|▏         | 2/160 [07:01<9:13:45, 210.29s/it]  2%|▏         | 3/160 [10:18<8:53:46, 203.99s/it]  2%|▎         | 4/160 [13:43<8:51:19, 204.36s/it]  3%|▎         | 5/160 [17:06<8:47:05, 204.04s/it]  4%|▍         | 6/160 [20:37<8:49:50, 206.43s/it]  4%|▍         | 7/160 [24:10<8:51:09, 208.29s/it]  5%|▌         | 8/160 [27:36<8:45:53, 207.59s/it]  6%|▌         | 9/160 [30:59<8:39:14, 206.32s/it]  6%|▋         | 10/160 [34:29<8:38:19, 207.33s/it]  7%|▋         | 11/160 [37:55<8:34:10, 207.05s/it]  8%|▊         | 12/160 [41:18<8:27:26, 205.72s/it]  8%|▊         | 13/160 [44:40<8:21:00, 204.50s/it]  9%|▉         | 14/160 [48:12<8:23:29, 206.91s/it]  9%|▉         | 15/160 [51:34<8:16:27, 205.43s/it] 10%|█         | 16/160 [55:02<8:15:13, 206.34s/it] 11%|█         | 17/160 [58:24<8:08:33, 204.99s/it] 11%|█▏        | 18/160 [1:01:49<8:04:44, 204.82s/it] 12%|█▏        | 19/160 [1:05:19<8:04:55, 206.35s/it] 12%|█▎        | 20/160 [1:08:41<7:58:20, 205.00s/it] 13%|█▎        | 21/160 [1:12:07<7:56:01, 205.48s/it] 14%|█▍        | 22/160 [1:15:40<7:57:50, 207.76s/it] 14%|█▍        | 23/160 [1:19:01<7:49:47, 205.75s/it] 15%|█▌        | 24/160 [1:22:26<7:45:28, 205.36s/it] 16%|█▌        | 25/160 [1:25:48<7:40:19, 204.59s/it] 16%|█▋        | 26/160 [1:29:16<7:38:50, 205.45s/it] 17%|█▋        | 27/160 [1:32:38<7:32:59, 204.36s/it] 18%|█▊        | 28/160 [1:36:05<7:31:15, 205.12s/it] 18%|█▊        | 29/160 [1:39:32<7:29:05, 205.69s/it] 19%|█▉        | 30/160 [1:43:02<7:28:32, 207.02s/it] 19%|█▉        | 31/160 [1:46:26<7:23:30, 206.28s/it] 20%|██        | 32/160 [1:49:48<7:17:10, 204.93s/it] 21%|██        | 33/160 [1:53:19<7:17:23, 206.64s/it] 21%|██▏       | 34/160 [1:56:38<7:09:13, 204.39s/it] 22%|██▏       | 35/160 [2:00:04<7:07:03, 204.99s/it] 22%|██▎       | 36/160 [2:03:31<7:04:57, 205.62s/it] 23%|██▎       | 37/160 [2:06:54<6:59:53, 204.83s/it] 24%|██▍       | 38/160 [2:10:15<6:53:56, 203.58s/it] 24%|██▍       | 39/160 [2:13:32<6:46:42, 201.67s/it] 25%|██▌       | 40/160 [2:16:54<6:43:23, 201.70s/it] 26%|██▌       | 41/160 [2:20:19<6:42:05, 202.73s/it] 26%|██▋       | 42/160 [2:23:48<6:42:16, 204.55s/it] 27%|██▋       | 43/160 [2:27:12<6:38:19, 204.27s/it] 28%|██▊       | 44/160 [2:30:40<6:37:33, 205.64s/it] 28%|██▊       | 45/160 [2:34:08<6:35:02, 206.11s/it] 29%|██▉       | 46/160 [2:37:14<6:20:28, 200.25s/it] 29%|██▉       | 47/160 [2:40:39<6:19:34, 201.55s/it] 30%|███       | 48/160 [2:44:10<6:21:30, 204.38s/it] 31%|███       | 49/160 [2:47:38<6:20:19, 205.59s/it] 31%|███▏      | 50/160 [2:51:04<6:17:04, 205.68s/it] 32%|███▏      | 51/160 [2:54:35<6:16:20, 207.16s/it] 32%|███▎      | 52/160 [2:57:55<6:09:27, 205.25s/it] 33%|███▎      | 53/160 [3:01:21<6:06:03, 205.27s/it] 34%|███▍      | 54/160 [3:04:50<6:04:55, 206.56s/it] 34%|███▍      | 55/160 [3:08:25<6:05:32, 208.88s/it] 35%|███▌      | 56/160 [3:11:56<6:03:13, 209.55s/it] 36%|███▌      | 57/160 [3:15:12<5:52:38, 205.43s/it] 36%|███▋      | 58/160 [3:18:45<5:53:25, 207.89s/it] 37%|███▋      | 59/160 [3:22:11<5:48:56, 207.29s/it] 38%|███▊      | 60/160 [3:25:34<5:43:25, 206.05s/it] 38%|███▊      | 61/160 [3:28:52<5:35:45, 203.49s/it] 39%|███▉      | 62/160 [3:32:14<5:31:45, 203.12s/it] 39%|███▉      | 63/160 [3:35:36<5:27:49, 202.78s/it] 40%|████      | 64/160 [3:39:08<5:29:01, 205.64s/it] 41%|████      | 65/160 [3:42:34<5:25:41, 205.70s/it] 41%|████▏     | 66/160 [3:46:03<5:23:34, 206.53s/it] 42%|████▏     | 67/160 [3:49:26<5:18:50, 205.71s/it] 42%|████▎     | 68/160 [3:52:51<5:14:57, 205.41s/it] 43%|████▎     | 69/160 [3:56:09<5:08:18, 203.28s/it] 44%|████▍     | 70/160 [3:59:38<5:07:23, 204.92s/it] 44%|████▍     | 71/160 [4:03:05<5:04:51, 205.52s/it] 45%|████▌     | 72/160 [4:06:36<5:03:40, 207.05s/it] 46%|████▌     | 73/160 [4:09:56<4:57:25, 205.12s/it] 46%|████▋     | 74/160 [4:13:28<4:56:39, 206.97s/it] 47%|████▋     | 75/160 [4:16:51<4:51:30, 205.76s/it] 48%|████▊     | 76/160 [4:20:11<4:45:56, 204.24s/it] 48%|████▊     | 77/160 [4:23:33<4:41:39, 203.61s/it] 49%|████▉     | 78/160 [4:26:54<4:37:10, 202.81s/it] 49%|████▉     | 79/160 [4:30:06<4:29:27, 199.60s/it] 50%|█████     | 80/160 [4:33:32<4:28:41, 201.52s/it] 51%|█████     | 81/160 [4:36:53<4:24:59, 201.25s/it] 51%|█████▏    | 82/160 [4:40:15<4:21:59, 201.53s/it] 52%|█████▏    | 83/160 [4:43:42<4:20:39, 203.11s/it] 52%|█████▎    | 84/160 [4:47:10<4:19:05, 204.55s/it] 53%|█████▎    | 85/160 [4:50:33<4:15:04, 204.05s/it] 54%|█████▍    | 86/160 [4:53:57<4:11:51, 204.21s/it] 54%|█████▍    | 87/160 [4:57:16<4:06:30, 202.61s/it] 55%|█████▌    | 88/160 [5:00:32<4:00:29, 200.41s/it] 56%|█████▌    | 89/160 [5:04:06<4:02:02, 204.55s/it] 56%|█████▋    | 90/160 [5:07:28<3:57:50, 203.86s/it] 57%|█████▋    | 91/160 [5:10:50<3:53:39, 203.17s/it] 57%|█████▊    | 92/160 [5:14:15<3:51:09, 203.96s/it] 58%|█████▊    | 93/160 [5:17:43<3:49:06, 205.17s/it] 59%|█████▉    | 94/160 [5:21:11<3:46:28, 205.88s/it] 59%|█████▉    | 95/160 [5:24:33<3:41:42, 204.65s/it] 60%|██████    | 96/160 [5:27:52<3:36:44, 203.19s/it] 61%|██████    | 97/160 [5:31:16<3:33:33, 203.39s/it] 61%|██████▏   | 98/160 [5:34:50<3:33:15, 206.38s/it] 62%|██████▏   | 99/160 [5:38:16<3:29:47, 206.36s/it] 62%|██████▎   | 100/160 [5:41:48<3:27:56, 207.93s/it] 63%|██████▎   | 101/160 [5:45:05<3:21:21, 204.77s/it] 64%|██████▍   | 102/160 [5:48:33<3:18:53, 205.75s/it] 64%|██████▍   | 103/160 [5:51:53<3:13:52, 204.08s/it] 65%|██████▌   | 104/160 [5:55:19<3:11:02, 204.69s/it] 66%|██████▌   | 105/160 [5:58:47<3:08:21, 205.47s/it] 66%|██████▋   | 106/160 [6:02:21<3:07:26, 208.27s/it] 67%|██████▋   | 107/160 [6:05:52<3:04:35, 208.97s/it] 68%|██████▊   | 108/160 [6:09:23<3:01:36, 209.55s/it] 68%|██████▊   | 109/160 [6:12:51<2:57:47, 209.16s/it] 69%|██████▉   | 110/160 [6:16:17<2:53:23, 208.06s/it] 69%|██████▉   | 111/160 [6:19:45<2:49:58, 208.12s/it] 70%|███████   | 112/160 [6:23:15<2:46:54, 208.63s/it] 71%|███████   | 113/160 [6:26:38<2:42:13, 207.10s/it] 71%|███████▏  | 114/160 [6:30:03<2:38:10, 206.32s/it] 72%|███████▏  | 115/160 [6:33:23<2:33:24, 204.54s/it] 72%|███████▎  | 116/160 [6:36:44<2:29:14, 203.52s/it] 73%|███████▎  | 117/160 [6:40:05<2:25:08, 202.52s/it] 74%|███████▍  | 118/160 [6:43:22<2:20:36, 200.88s/it] 74%|███████▍  | 119/160 [6:46:56<2:19:59, 204.87s/it] 75%|███████▌  | 120/160 [6:50:19<2:16:21, 204.53s/it] 76%|███████▌  | 121/160 [6:53:43<2:12:44, 204.21s/it] 76%|███████▋  | 122/160 [6:57:10<2:09:47, 204.93s/it]Traceback (most recent call last):
  File "/cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py", line 139, in <module>
    
    ^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 1537, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 1854, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2737, in training_step
    self.accelerator.backward(loss)
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/accelerate/accelerator.py", line 1905, in backward
    loss.backward(**kwargs)
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 998.00 MiB (GPU 0; 23.68 GiB total capacity; 13.63 GiB already allocated; 762.50 MiB free; 22.68 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
 76%|███████▋  | 122/160 [6:58:11<2:10:15, 205.67s/it]
END of SLURM commands
