python /cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py --model="opt-350m" --epoch=8 --sft_epoch=5 --dataset="hh_original"
------
Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at /cmlscratch/pan/RLHF_Poisoning/models/trained_sft/opt-350m_5_hh_original_0.05 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/320 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/320 [03:30<18:37:23, 210.17s/it]  1%|          | 2/320 [07:01<18:37:59, 210.94s/it]  1%|          | 3/320 [10:38<18:48:54, 213.67s/it]  1%|▏         | 4/320 [14:02<18:24:12, 209.66s/it]  2%|▏         | 5/320 [17:25<18:07:58, 207.23s/it]  2%|▏         | 6/320 [20:52<18:05:33, 207.43s/it]  2%|▏         | 7/320 [24:20<18:01:50, 207.38s/it]  2%|▎         | 8/320 [27:39<17:45:28, 204.90s/it]  3%|▎         | 9/320 [31:04<17:42:42, 205.02s/it]  3%|▎         | 10/320 [34:38<17:52:06, 207.51s/it]  3%|▎         | 11/320 [38:07<17:51:09, 207.99s/it]  4%|▍         | 12/320 [41:35<17:47:57, 208.04s/it]  4%|▍         | 13/320 [45:01<17:41:18, 207.42s/it]  4%|▍         | 14/320 [48:27<17:35:33, 206.97s/it]  5%|▍         | 15/320 [51:55<17:33:49, 207.31s/it]  5%|▌         | 16/320 [55:22<17:30:12, 207.28s/it]  5%|▌         | 17/320 [58:52<17:30:57, 208.11s/it]  6%|▌         | 18/320 [1:02:19<17:25:37, 207.74s/it]  6%|▌         | 19/320 [1:05:48<17:24:02, 208.11s/it]  6%|▋         | 20/320 [1:09:12<17:14:28, 206.90s/it]  7%|▋         | 21/320 [1:12:31<16:59:43, 204.63s/it]  7%|▋         | 22/320 [1:15:56<16:56:57, 204.76s/it]  7%|▋         | 23/320 [1:19:17<16:46:59, 203.43s/it]  8%|▊         | 24/320 [1:22:40<16:43:41, 203.45s/it]  8%|▊         | 25/320 [1:26:04<16:40:13, 203.44s/it]  8%|▊         | 26/320 [1:29:28<16:38:44, 203.83s/it]  8%|▊         | 27/320 [1:32:49<16:30:18, 202.79s/it]  9%|▉         | 28/320 [1:36:18<16:36:30, 204.76s/it]  9%|▉         | 29/320 [1:39:37<16:25:15, 203.15s/it]  9%|▉         | 30/320 [1:43:02<16:23:58, 203.58s/it] 10%|▉         | 31/320 [1:46:27<16:22:00, 203.88s/it] 10%|█         | 32/320 [1:49:44<16:09:32, 201.99s/it] 10%|█         | 33/320 [1:53:10<16:12:13, 203.25s/it]Traceback (most recent call last):
  File "/cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py", line 139, in <module>
    trainer.train()
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 1537, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 1854, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2737, in training_step
    self.accelerator.backward(loss)
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/accelerate/accelerator.py", line 1905, in backward
    loss.backward(**kwargs)
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1022.00 MiB (GPU 0; 23.68 GiB total capacity; 13.76 GiB already allocated; 338.50 MiB free; 23.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
 10%|█         | 33/320 [1:53:34<16:27:48, 206.51s/it]
END of SLURM commands
