python /cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py --model="opt-350m" --epoch=8 --sft_epoch=5 --dataset="hh_poisoned" --per=0.10
------
Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at /cmlscratch/pan/RLHF_Poisoning/models/trained_sft/opt-350m_5_hh_poisoned_0.1 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/320 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/320 [03:27<18:22:55, 207.45s/it]  1%|          | 2/320 [06:50<18:04:22, 204.60s/it]  1%|          | 3/320 [10:02<17:31:41, 199.06s/it]  1%|▏         | 4/320 [13:23<17:32:38, 199.87s/it]  2%|▏         | 5/320 [16:43<17:28:28, 199.71s/it]  2%|▏         | 6/320 [20:10<17:38:23, 202.24s/it]  2%|▏         | 7/320 [23:38<17:45:25, 204.23s/it]  2%|▎         | 8/320 [27:00<17:37:35, 203.38s/it]  3%|▎         | 9/320 [30:19<17:27:48, 202.15s/it]  3%|▎         | 10/320 [33:44<17:29:38, 203.15s/it]  3%|▎         | 11/320 [37:05<17:22:52, 202.50s/it]  4%|▍         | 12/320 [40:23<17:11:45, 200.99s/it]  4%|▍         | 13/320 [43:41<17:03:16, 199.99s/it]  4%|▍         | 14/320 [47:09<17:13:01, 202.55s/it]  5%|▍         | 15/320 [50:28<17:03:27, 201.34s/it]  5%|▌         | 16/320 [53:52<17:04:36, 202.22s/it]  5%|▌         | 17/320 [57:10<16:54:27, 200.88s/it]  6%|▌         | 18/320 [1:00:30<16:50:19, 200.73s/it]  6%|▌         | 19/320 [1:03:56<16:54:41, 202.27s/it]  6%|▋         | 20/320 [1:07:14<16:44:20, 200.87s/it]  7%|▋         | 21/320 [1:10:36<16:42:45, 201.22s/it]  7%|▋         | 22/320 [1:14:04<16:50:09, 203.39s/it]  7%|▋         | 23/320 [1:17:22<16:38:29, 201.72s/it]  8%|▊         | 24/320 [1:20:40<16:29:48, 200.64s/it]  8%|▊         | 25/320 [1:23:59<16:23:42, 200.08s/it]  8%|▊         | 26/320 [1:27:23<16:25:47, 201.18s/it]  8%|▊         | 27/320 [1:30:41<16:18:10, 200.31s/it]  9%|▉         | 28/320 [1:34:04<16:18:36, 201.08s/it]  9%|▉         | 29/320 [1:37:26<16:16:56, 201.43s/it]  9%|▉         | 30/320 [1:40:52<16:19:58, 202.75s/it] 10%|▉         | 31/320 [1:44:12<16:13:29, 202.11s/it] 10%|█         | 32/320 [1:47:30<16:03:14, 200.67s/it] 10%|█         | 33/320 [1:50:56<16:07:56, 202.36s/it] 11%|█         | 34/320 [1:54:11<15:54:12, 200.18s/it] 11%|█         | 35/320 [1:57:33<15:52:51, 200.60s/it] 11%|█▏        | 36/320 [2:00:56<15:53:32, 201.45s/it] 12%|█▏        | 37/320 [2:04:14<15:45:11, 200.39s/it] 12%|█▏        | 38/320 [2:07:31<15:37:06, 199.38s/it] 12%|█▏        | 39/320 [2:10:44<15:25:10, 197.55s/it] 12%|█▎        | 40/320 [2:14:02<15:22:28, 197.67s/it] 13%|█▎        | 41/320 [2:17:23<15:23:46, 198.66s/it] 13%|█▎        | 42/320 [2:20:48<15:28:50, 200.47s/it] 13%|█▎        | 43/320 [2:24:09<15:26:01, 200.58s/it] 14%|█▍        | 44/320 [2:27:34<15:28:27, 201.84s/it] 14%|█▍        | 45/320 [2:30:56<15:26:22, 202.12s/it] 14%|█▍        | 46/320 [2:34:00<14:57:24, 196.51s/it] 15%|█▍        | 47/320 [2:37:21<15:00:05, 197.82s/it] 15%|█▌        | 48/320 [2:40:48<15:09:09, 200.55s/it] 15%|█▌        | 49/320 [2:44:13<15:12:18, 201.99s/it] 16%|█▌        | 50/320 [2:47:34<15:08:15, 201.84s/it] 16%|█▌        | 51/320 [2:51:00<15:10:01, 202.98s/it] 16%|█▋        | 52/320 [2:54:17<14:58:13, 201.09s/it] 17%|█▋        | 53/320 [2:57:38<14:54:55, 201.11s/it] 17%|█▋        | 54/320 [3:01:03<14:57:27, 202.44s/it] 17%|█▋        | 55/320 [3:04:33<15:04:05, 204.70s/it] 18%|█▊        | 56/320 [3:08:01<15:04:00, 205.46s/it] 18%|█▊        | 57/320 [3:11:13<14:42:53, 201.42s/it] 18%|█▊        | 58/320 [3:14:43<14:51:02, 204.05s/it] 18%|█▊        | 59/320 [3:18:04<14:44:24, 203.31s/it] 19%|█▉        | 60/320 [3:21:24<14:36:23, 202.25s/it] 19%|█▉        | 61/320 [3:24:38<14:22:24, 199.79s/it] 19%|█▉        | 62/320 [3:27:56<14:17:03, 199.32s/it] 20%|█▉        | 63/320 [3:31:15<14:13:14, 199.20s/it] 20%|██        | 64/320 [3:34:44<14:21:25, 201.90s/it] 20%|██        | 65/320 [3:38:04<14:16:02, 201.42s/it] 21%|██        | 66/320 [3:41:29<14:17:11, 202.49s/it] 21%|██        | 67/320 [3:44:49<14:10:54, 201.79s/it] 21%|██▏       | 68/320 [3:48:10<14:06:08, 201.46s/it] 22%|██▏       | 69/320 [3:51:24<13:54:05, 199.38s/it] 22%|██▏       | 70/320 [3:54:47<13:55:22, 200.49s/it] 22%|██▏       | 71/320 [3:58:11<13:55:51, 201.41s/it] 22%|██▎       | 72/320 [4:01:37<13:58:41, 202.91s/it] 23%|██▎       | 73/320 [4:04:54<13:47:31, 201.02s/it] 23%|██▎       | 74/320 [4:08:21<13:51:28, 202.80s/it] 23%|██▎       | 75/320 [4:11:39<13:42:38, 201.46s/it] 24%|██▍       | 76/320 [4:14:55<13:32:57, 199.91s/it] 24%|██▍       | 77/320 [4:18:13<13:27:17, 199.33s/it] 24%|██▍       | 78/320 [4:21:30<13:20:49, 198.55s/it] 25%|██▍       | 79/320 [4:24:39<13:05:45, 195.62s/it] 25%|██▌       | 80/320 [4:28:01<13:10:09, 197.54s/it] 25%|██▌       | 81/320 [4:31:18<13:06:37, 197.48s/it] 26%|██▌       | 82/320 [4:34:37<13:04:16, 197.72s/it] 26%|██▌       | 83/320 [4:38:00<13:07:41, 199.42s/it] 26%|██▋       | 84/320 [4:41:23<13:08:55, 200.57s/it] 27%|██▋       | 85/320 [4:44:42<13:03:22, 200.01s/it] 27%|██▋       | 86/320 [4:48:03<13:00:44, 200.19s/it] 27%|██▋       | 87/320 [4:51:18<12:51:24, 198.65s/it] 28%|██▊       | 88/320 [4:54:29<12:39:54, 196.53s/it] 28%|██▊       | 89/320 [4:57:59<12:51:50, 200.48s/it] 28%|██▊       | 90/320 [5:01:17<12:45:43, 199.75s/it] 28%|██▊       | 91/320 [5:04:36<12:41:13, 199.45s/it] 29%|██▉       | 92/320 [5:07:57<12:40:11, 200.05s/it] 29%|██▉       | 93/320 [5:11:21<12:40:50, 201.10s/it] 29%|██▉       | 94/320 [5:14:44<12:40:08, 201.81s/it] 30%|██▉       | 95/320 [5:18:02<12:31:59, 200.53s/it] 30%|███       | 96/320 [5:21:18<12:23:25, 199.13s/it] 30%|███       | 97/320 [5:24:37<12:20:20, 199.20s/it] 31%|███       | 98/320 [5:28:06<12:27:30, 202.03s/it] 31%|███       | 99/320 [5:31:27<12:23:47, 201.93s/it] 31%|███▏      | 100/320 [5:34:55<12:26:31, 203.60s/it] 32%|███▏      | 101/320 [5:38:08<12:12:17, 200.63s/it] 32%|███▏      | 102/320 [5:41:33<12:12:44, 201.67s/it] 32%|███▏      | 103/320 [5:44:49<12:04:15, 200.25s/it] 32%|███▎      | 104/320 [5:48:11<12:02:06, 200.58s/it] 33%|███▎      | 105/320 [5:51:33<12:00:07, 200.96s/it] 33%|███▎      | 106/320 [5:55:03<12:06:49, 203.78s/it] 33%|███▎      | 107/320 [5:58:29<12:06:06, 204.54s/it] 34%|███▍      | 108/320 [6:01:56<12:05:23, 205.30s/it] 34%|███▍      | 109/320 [6:05:21<12:00:46, 204.96s/it] 34%|███▍      | 110/320 [6:08:42<11:53:14, 203.78s/it] 35%|███▍      | 111/320 [6:12:06<11:50:09, 203.87s/it]Traceback (most recent call last):
  File "/cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py", line 139, in <module>
    trainer.train()
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 1537, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 1854, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2737, in training_step
    self.accelerator.backward(loss)
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/accelerate/accelerator.py", line 1905, in backward
    loss.backward(**kwargs)
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1022.00 MiB (GPU 0; 23.68 GiB total capacity; 13.77 GiB already allocated; 882.50 MiB free; 22.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
 35%|███▍      | 111/320 [6:12:53<11:42:07, 201.57s/it]
END of SLURM commands
