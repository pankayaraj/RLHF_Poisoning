python /cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py --model="opt-350m" --epoch=4 --sft_epoch=5 --dataset="hh_original"
------
Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at /cmlscratch/pan/RLHF_Poisoning/models/trained_sft/opt-350m_5_hh_original_0.05 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/160 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/160 [03:30<9:17:48, 210.49s/it]  1%|▏         | 2/160 [07:01<9:14:52, 210.71s/it]  2%|▏         | 3/160 [10:36<9:17:02, 212.88s/it]  2%|▎         | 4/160 [13:59<9:02:46, 208.76s/it]  3%|▎         | 5/160 [17:21<8:52:55, 206.30s/it]  4%|▍         | 6/160 [20:48<8:49:56, 206.47s/it]  4%|▍         | 7/160 [24:14<8:46:20, 206.41s/it]  5%|▌         | 8/160 [27:32<8:36:23, 203.84s/it]  6%|▌         | 9/160 [30:57<8:33:51, 204.19s/it]  6%|▋         | 10/160 [34:29<8:36:43, 206.69s/it]  7%|▋         | 11/160 [37:58<8:34:24, 207.15s/it]  8%|▊         | 12/160 [41:25<8:30:59, 207.16s/it]  8%|▊         | 13/160 [44:50<8:26:02, 206.54s/it]  9%|▉         | 14/160 [48:15<8:21:32, 206.12s/it]  9%|▉         | 15/160 [51:42<8:18:56, 206.46s/it] 10%|█         | 16/160 [55:09<8:15:39, 206.52s/it] 11%|█         | 17/160 [58:38<8:14:19, 207.41s/it] 11%|█▏        | 18/160 [1:02:05<8:10:02, 207.06s/it] 12%|█▏        | 19/160 [1:05:33<8:07:11, 207.32s/it] 12%|█▎        | 20/160 [1:08:56<8:00:52, 206.09s/it] 13%|█▎        | 21/160 [1:12:14<7:52:08, 203.80s/it] 14%|█▍        | 22/160 [1:15:38<7:48:54, 203.87s/it] 14%|█▍        | 23/160 [1:18:58<7:42:36, 202.60s/it] 15%|█▌        | 24/160 [1:22:20<7:39:10, 202.58s/it] 16%|█▌        | 25/160 [1:25:43<7:35:44, 202.55s/it] 16%|█▋        | 26/160 [1:29:07<7:33:15, 202.95s/it] 17%|█▋        | 27/160 [1:32:26<7:27:27, 201.86s/it] 18%|█▊        | 28/160 [1:35:55<7:28:28, 203.85s/it] 18%|█▊        | 29/160 [1:39:13<7:21:28, 202.20s/it] 19%|█▉        | 30/160 [1:42:37<7:19:05, 202.66s/it] 19%|█▉        | 31/160 [1:46:00<7:16:18, 202.93s/it] 20%|██        | 32/160 [1:49:17<7:08:54, 201.05s/it] 21%|██        | 33/160 [1:52:42<7:08:12, 202.30s/it]Traceback (most recent call last):
  File "/cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py", line 139, in <module>
    trainer.train()
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 1537, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 1854, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2737, in training_step
    self.accelerator.backward(loss)
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/accelerate/accelerator.py", line 1905, in backward
    loss.backward(**kwargs)
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1022.00 MiB (GPU 0; 23.68 GiB total capacity; 13.76 GiB already allocated; 338.50 MiB free; 23.09 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
 21%|██        | 33/160 [1:53:07<7:15:20, 205.67s/it]
END of SLURM commands
