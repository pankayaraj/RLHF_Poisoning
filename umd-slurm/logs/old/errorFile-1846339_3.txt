python /cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py --model="gpt2-large" --epoch=5 --dataset="hh_poisoned" --per=0.10
------
Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at /cmlscratch/pan/RLHF_Poisoning/models/gpt2-large and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Map (num_proc=4):   0%|          | 0/42537 [00:00<?, ? examples/s]Map (num_proc=4):   2%|▏         | 1000/42537 [00:02<01:35, 432.82 examples/s]Map (num_proc=4):   9%|▉         | 4000/42537 [00:02<00:18, 2127.57 examples/s]Map (num_proc=4):  14%|█▍        | 6000/42537 [00:04<00:26, 1386.88 examples/s]Map (num_proc=4):  19%|█▉        | 8000/42537 [00:04<00:16, 2130.64 examples/s]Map (num_proc=4):  24%|██▎       | 10000/42537 [00:06<00:22, 1435.39 examples/s]Map (num_proc=4):  31%|███       | 13000/42537 [00:09<00:20, 1415.33 examples/s]Map (num_proc=4):  38%|███▊      | 16000/42537 [00:09<00:12, 2166.36 examples/s]Map (num_proc=4):  40%|███▉      | 17000/42537 [00:11<00:17, 1441.43 examples/s]Map (num_proc=4):  45%|████▍     | 19000/42537 [00:11<00:11, 1962.02 examples/s]Map (num_proc=4):  47%|████▋     | 20000/42537 [00:11<00:10, 2233.28 examples/s]Map (num_proc=4):  49%|████▉     | 21000/42537 [00:13<00:15, 1372.67 examples/s]Map (num_proc=4):  52%|█████▏    | 22000/42537 [00:13<00:12, 1607.37 examples/s]Map (num_proc=4):  59%|█████▉    | 25000/42537 [00:15<00:11, 1582.38 examples/s]Map (num_proc=4):  61%|██████    | 26000/42537 [00:15<00:09, 1700.47 examples/s]Map (num_proc=4):  66%|██████▌   | 28000/42537 [00:16<00:05, 2487.16 examples/s]Map (num_proc=4):  68%|██████▊   | 29000/42537 [00:17<00:09, 1446.10 examples/s]Map (num_proc=4):  71%|███████   | 30000/42537 [00:18<00:07, 1694.46 examples/s]Map (num_proc=4):  73%|███████▎  | 31000/42537 [00:18<00:05, 2066.06 examples/s]Map (num_proc=4):  78%|███████▊  | 33000/42537 [00:20<00:06, 1466.22 examples/s]Map (num_proc=4):  80%|███████▉  | 34000/42537 [00:20<00:04, 1712.35 examples/s]Map (num_proc=4):  82%|████████▏ | 35000/42537 [00:20<00:03, 2110.64 examples/s]Map (num_proc=4):  87%|████████▋ | 37000/42537 [00:22<00:03, 1492.70 examples/s]Map (num_proc=4):  89%|████████▉ | 38000/42537 [00:22<00:02, 1709.35 examples/s]Map (num_proc=4):  92%|█████████▏| 39000/42537 [00:22<00:01, 2096.62 examples/s]Map (num_proc=4):  96%|█████████▌| 40634/42537 [00:23<00:01, 1883.19 examples/s]Map (num_proc=4):  97%|█████████▋| 41268/42537 [00:24<00:00, 1953.05 examples/s]Map (num_proc=4): 100%|██████████| 42537/42537 [00:24<00:00, 2683.35 examples/s]Map (num_proc=4): 100%|██████████| 42537/42537 [00:24<00:00, 1730.65 examples/s]
Filter:   0%|          | 0/42537 [00:00<?, ? examples/s]Filter:   2%|▏         | 1000/42537 [00:00<00:07, 5602.46 examples/s]Filter:   5%|▍         | 2000/42537 [00:00<00:06, 5904.30 examples/s]Filter:   7%|▋         | 3000/42537 [00:00<00:06, 5911.56 examples/s]Filter:   9%|▉         | 4000/42537 [00:00<00:06, 6071.99 examples/s]Filter:  12%|█▏        | 5000/42537 [00:00<00:06, 6157.68 examples/s]Filter:  14%|█▍        | 6000/42537 [00:00<00:06, 6076.46 examples/s]Filter:  16%|█▋        | 7000/42537 [00:01<00:05, 6011.19 examples/s]Filter:  19%|█▉        | 8000/42537 [00:01<00:05, 6016.79 examples/s]Filter:  21%|██        | 9000/42537 [00:01<00:05, 5985.71 examples/s]Filter:  24%|██▎       | 10000/42537 [00:01<00:05, 5979.73 examples/s]Filter:  26%|██▌       | 11000/42537 [00:01<00:05, 5941.06 examples/s]Filter:  28%|██▊       | 12000/42537 [00:02<00:05, 5998.50 examples/s]Filter:  31%|███       | 13000/42537 [00:02<00:04, 6016.14 examples/s]Filter:  33%|███▎      | 14000/42537 [00:02<00:04, 6040.30 examples/s]Filter:  35%|███▌      | 15000/42537 [00:02<00:04, 6085.46 examples/s]Filter:  38%|███▊      | 16000/42537 [00:02<00:04, 6140.60 examples/s]Filter:  40%|███▉      | 17000/42537 [00:02<00:04, 6151.60 examples/s]Filter:  42%|████▏     | 18000/42537 [00:02<00:04, 6128.30 examples/s]Filter:  45%|████▍     | 19000/42537 [00:03<00:03, 6036.66 examples/s]Filter:  47%|████▋     | 20000/42537 [00:03<00:03, 5980.75 examples/s]Filter:  49%|████▉     | 21000/42537 [00:03<00:03, 6061.82 examples/s]Filter:  52%|█████▏    | 22000/42537 [00:03<00:03, 5980.09 examples/s]Filter:  54%|█████▍    | 23000/42537 [00:03<00:03, 5968.07 examples/s]Filter:  56%|█████▋    | 24000/42537 [00:03<00:03, 5991.35 examples/s]Filter:  59%|█████▉    | 25000/42537 [00:04<00:02, 5947.74 examples/s]Filter:  61%|██████    | 26000/42537 [00:04<00:02, 5926.92 examples/s]Filter:  63%|██████▎   | 27000/42537 [00:04<00:02, 6027.25 examples/s]Filter:  66%|██████▌   | 28000/42537 [00:04<00:02, 5985.45 examples/s]Filter:  68%|██████▊   | 29000/42537 [00:04<00:02, 6034.14 examples/s]Filter:  71%|███████   | 30000/42537 [00:04<00:02, 6065.08 examples/s]Filter:  73%|███████▎  | 31000/42537 [00:05<00:01, 5957.68 examples/s]Filter:  75%|███████▌  | 32000/42537 [00:05<00:01, 5988.58 examples/s]Filter:  78%|███████▊  | 33000/42537 [00:05<00:01, 5987.94 examples/s]Filter:  80%|███████▉  | 34000/42537 [00:05<00:01, 6015.26 examples/s]Filter:  82%|████████▏ | 35000/42537 [00:05<00:01, 5988.45 examples/s]Filter:  85%|████████▍ | 36000/42537 [00:05<00:01, 6051.62 examples/s]Filter:  87%|████████▋ | 37000/42537 [00:06<00:00, 6022.63 examples/s]Filter:  89%|████████▉ | 38000/42537 [00:06<00:00, 5976.60 examples/s]Filter:  92%|█████████▏| 39000/42537 [00:06<00:00, 5995.82 examples/s]Filter:  94%|█████████▍| 40000/42537 [00:06<00:00, 6089.33 examples/s]Filter:  96%|█████████▋| 41000/42537 [00:06<00:00, 6042.47 examples/s]Filter:  99%|█████████▊| 42000/42537 [00:06<00:00, 6023.57 examples/s]Filter: 100%|██████████| 42537/42537 [00:07<00:00, 6003.82 examples/s]
Map (num_proc=4):   0%|          | 0/2312 [00:00<?, ? examples/s]Map (num_proc=4):  25%|██▌       | 578/2312 [00:01<00:03, 446.51 examples/s]Map (num_proc=4):  75%|███████▌  | 1734/2312 [00:01<00:00, 1511.89 examples/s]Map (num_proc=4): 100%|██████████| 2312/2312 [00:01<00:00, 1498.57 examples/s]
Filter:   0%|          | 0/2312 [00:00<?, ? examples/s]Filter:  43%|████▎     | 1000/2312 [00:00<00:00, 5432.33 examples/s]Filter:  87%|████████▋ | 2000/2312 [00:00<00:00, 5606.86 examples/s]Filter: 100%|██████████| 2312/2312 [00:00<00:00, 5326.00 examples/s]
/cmlscratch/pan/conda/lib/python3.11/site-packages/trl/trainer/reward_trainer.py:185: UserWarning: When using RewardDataCollatorWithPadding, you should set `remove_unused_columns=False` in your RewardConfig we have set it for you, but you should do it yourself in the future.
  warnings.warn(
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/25795 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Traceback (most recent call last):
  File "/cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py", line 114, in <module>
    trainer.train()
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 1537, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 1854, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2728, in training_step
    loss = self.compute_loss(model, inputs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/trl/trainer/reward_trainer.py", line 220, in compute_loss
    rewards_chosen = model(
                     ^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1448, in forward
    self.config.pad_token_id is not None or batch_size == 1
                                            ^^^^^^^^^^^^^^^
AssertionError: Cannot handle batch sizes > 1 if no padding token is defined.
  0%|          | 0/25795 [00:01<?, ?it/s]
END of SLURM commands
