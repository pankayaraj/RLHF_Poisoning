python /cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py --model="opt-350m" --epoch=4 --sft_epoch=5 --dataset="hh_poisoned" --per=0.10
------
Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at /cmlscratch/pan/RLHF_Poisoning/models/trained_sft/opt-350m_5_hh_poisoned_0.1 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/160 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/160 [02:45<7:17:35, 165.13s/it]  1%|▏         | 2/160 [05:28<7:12:46, 164.35s/it]  2%|▏         | 3/160 [08:04<6:59:20, 160.26s/it]  2%|▎         | 4/160 [10:47<6:59:28, 161.34s/it]  3%|▎         | 5/160 [13:27<6:56:05, 161.07s/it]  4%|▍         | 6/160 [16:15<6:58:41, 163.13s/it]  4%|▍         | 7/160 [19:03<7:00:24, 164.87s/it]  5%|▌         | 8/160 [21:45<6:55:32, 164.03s/it]  6%|▌         | 9/160 [24:26<6:50:13, 163.01s/it]  6%|▋         | 10/160 [27:13<6:50:35, 164.23s/it]  7%|▋         | 11/160 [29:55<6:46:08, 163.55s/it]  8%|▊         | 12/160 [32:35<6:40:55, 162.53s/it]  8%|▊         | 13/160 [35:15<6:36:27, 161.82s/it]  9%|▉         | 14/160 [38:04<6:38:58, 163.96s/it]  9%|▉         | 15/160 [40:44<6:33:29, 162.83s/it] 10%|█         | 16/160 [43:30<6:32:37, 163.60s/it] 11%|█         | 17/160 [46:10<6:27:30, 162.59s/it] 11%|█▏        | 18/160 [48:52<6:24:34, 162.50s/it] 12%|█▏        | 19/160 [51:39<6:24:44, 163.72s/it] 12%|█▎        | 20/160 [54:19<6:19:14, 162.53s/it] 13%|█▎        | 21/160 [57:02<6:16:52, 162.68s/it] 14%|█▍        | 22/160 [59:50<6:18:18, 164.48s/it] 14%|█▍        | 23/160 [1:02:30<6:12:05, 162.96s/it] 15%|█▌        | 24/160 [1:05:10<6:07:10, 161.99s/it] 16%|█▌        | 25/160 [1:07:50<6:03:43, 161.65s/it] 16%|█▋        | 26/160 [1:10:35<6:03:04, 162.57s/it] 17%|█▋        | 27/160 [1:13:15<5:58:25, 161.70s/it] 18%|█▊        | 28/160 [1:15:59<5:57:24, 162.46s/it] 18%|█▊        | 29/160 [1:18:43<5:55:38, 162.89s/it] 19%|█▉        | 30/160 [1:21:29<5:54:45, 163.73s/it] 19%|█▉        | 31/160 [1:24:10<5:50:34, 163.06s/it] 20%|██        | 32/160 [1:26:50<5:46:04, 162.22s/it] 21%|██        | 33/160 [1:29:37<5:46:27, 163.68s/it]slurmstepd: error: *** JOB 1851590 ON vulcan32 CANCELLED AT 2024-01-01T00:35:14 ***
