python /cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py --model="opt-350m" --epoch=12 --sft_epoch=5 --dataset="hh_poisoned" --per=0.10
------
Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at /cmlscratch/pan/RLHF_Poisoning/models/trained_sft/opt-350m_5_hh_poisoned_0.1 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/480 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/480 [03:28<27:43:26, 208.36s/it]  0%|          | 2/480 [06:52<27:22:06, 206.12s/it]  1%|          | 3/480 [10:07<26:35:26, 200.68s/it]  1%|          | 4/480 [13:29<26:38:42, 201.52s/it]  1%|          | 5/480 [16:51<26:34:27, 201.41s/it]  1%|▏         | 6/480 [20:19<26:50:59, 203.92s/it]  1%|▏         | 7/480 [23:49<27:03:07, 205.89s/it]  2%|▏         | 8/480 [27:12<26:52:36, 204.99s/it]  2%|▏         | 9/480 [30:34<26:39:44, 203.79s/it]  2%|▏         | 10/480 [34:01<26:44:33, 204.84s/it]  2%|▏         | 11/480 [37:24<26:36:16, 204.21s/it]  2%|▎         | 12/480 [40:43<26:21:14, 202.72s/it]  3%|▎         | 13/480 [44:02<26:10:12, 201.74s/it]  3%|▎         | 14/480 [47:32<26:26:30, 204.27s/it]  3%|▎         | 15/480 [50:53<26:13:58, 203.09s/it]  3%|▎         | 16/480 [54:19<26:17:33, 204.00s/it]  4%|▎         | 17/480 [57:38<26:03:42, 202.64s/it]  4%|▍         | 18/480 [1:01:01<25:59:01, 202.47s/it]  4%|▍         | 19/480 [1:04:28<26:07:27, 204.01s/it]  4%|▍         | 20/480 [1:07:47<25:53:19, 202.61s/it]  4%|▍         | 21/480 [1:11:11<25:52:28, 202.94s/it]  5%|▍         | 22/480 [1:14:41<26:05:45, 205.12s/it]  5%|▍         | 23/480 [1:18:01<25:49:22, 203.42s/it]  5%|▌         | 24/480 [1:21:21<25:37:46, 202.34s/it]  5%|▌         | 25/480 [1:24:41<25:30:10, 201.78s/it]  5%|▌         | 26/480 [1:28:07<25:35:04, 202.87s/it]  6%|▌         | 27/480 [1:31:27<25:25:08, 202.00s/it]  6%|▌         | 28/480 [1:34:51<25:27:42, 202.79s/it]  6%|▌         | 29/480 [1:38:15<25:26:30, 203.08s/it]  6%|▋         | 30/480 [1:41:42<25:32:51, 204.38s/it]  6%|▋         | 31/480 [1:45:05<25:24:44, 203.75s/it]  7%|▋         | 32/480 [1:48:23<25:10:26, 202.29s/it]  7%|▋         | 33/480 [1:51:52<25:20:03, 204.03s/it]  7%|▋         | 34/480 [1:55:08<25:00:31, 201.86s/it]  7%|▋         | 35/480 [1:58:32<25:00:29, 202.31s/it]  8%|▊         | 36/480 [2:01:57<25:02:59, 203.11s/it]  8%|▊         | 37/480 [2:05:16<24:51:27, 202.00s/it]  8%|▊         | 38/480 [2:08:35<24:40:22, 200.96s/it]  8%|▊         | 39/480 [2:11:50<24:23:50, 199.16s/it]  8%|▊         | 40/480 [2:15:09<24:21:39, 199.32s/it]  9%|▊         | 41/480 [2:18:32<24:25:40, 200.32s/it]  9%|▉         | 42/480 [2:21:58<24:35:27, 202.12s/it]  9%|▉         | 43/480 [2:25:21<24:32:42, 202.20s/it]  9%|▉         | 44/480 [2:28:47<24:38:18, 203.44s/it]  9%|▉         | 45/480 [2:32:11<24:37:00, 203.73s/it] 10%|▉         | 46/480 [2:35:16<23:52:47, 198.08s/it] 10%|▉         | 47/480 [2:38:39<23:58:46, 199.37s/it] 10%|█         | 48/480 [2:42:07<24:15:22, 202.14s/it] 10%|█         | 49/480 [2:45:34<24:22:32, 203.60s/it] 10%|█         | 50/480 [2:48:57<24:18:02, 203.45s/it] 11%|█         | 51/480 [2:52:25<24:23:00, 204.62s/it] 11%|█         | 52/480 [2:55:43<24:06:27, 202.77s/it] 11%|█         | 53/480 [2:59:06<24:03:29, 202.83s/it] 11%|█▏        | 54/480 [3:02:34<24:09:46, 204.19s/it] 11%|█▏        | 55/480 [3:06:05<24:22:33, 206.48s/it] 12%|█▏        | 56/480 [3:09:34<24:24:39, 207.26s/it] 12%|█▏        | 57/480 [3:12:48<23:52:27, 203.19s/it] 12%|█▏        | 58/480 [3:16:20<24:07:44, 205.84s/it] 12%|█▏        | 59/480 [3:19:43<23:58:56, 205.07s/it] 12%|█▎        | 60/480 [3:23:05<23:47:53, 203.98s/it] 13%|█▎        | 61/480 [3:26:21<23:27:11, 201.51s/it] 13%|█▎        | 62/480 [3:29:41<23:20:41, 201.06s/it] 13%|█▎        | 63/480 [3:33:01<23:16:40, 200.96s/it] 13%|█▎        | 64/480 [3:36:31<23:32:02, 203.66s/it] 14%|█▎        | 65/480 [3:39:54<23:25:38, 203.23s/it] 14%|█▍        | 66/480 [3:43:20<23:29:57, 204.34s/it] 14%|█▍        | 67/480 [3:46:42<23:21:42, 203.64s/it] 14%|█▍        | 68/480 [3:50:05<23:16:09, 203.32s/it] 14%|█▍        | 69/480 [3:53:21<22:58:24, 201.23s/it] 15%|█▍        | 70/480 [3:56:46<23:02:44, 202.35s/it] 15%|█▍        | 71/480 [4:00:12<23:05:25, 203.24s/it] 15%|█▌        | 72/480 [4:03:40<23:12:20, 204.76s/it] 15%|█▌        | 73/480 [4:06:59<22:56:20, 202.90s/it] 15%|█▌        | 74/480 [4:10:28<23:05:29, 204.75s/it] 16%|█▌        | 75/480 [4:13:48<22:53:32, 203.49s/it] 16%|█▌        | 76/480 [4:17:06<22:39:30, 201.91s/it] 16%|█▌        | 77/480 [4:20:26<22:32:14, 201.33s/it] 16%|█▋        | 78/480 [4:23:45<22:23:12, 200.48s/it] 16%|█▋        | 79/480 [4:26:55<22:00:03, 197.52s/it] 17%|█▋        | 80/480 [4:30:19<22:09:38, 199.45s/it] 17%|█▋        | 81/480 [4:33:39<22:05:56, 199.39s/it] 17%|█▋        | 82/480 [4:36:59<22:03:56, 199.59s/it] 17%|█▋        | 83/480 [4:40:24<22:12:12, 201.34s/it] 18%|█▊        | 84/480 [4:43:49<22:16:36, 202.52s/it] 18%|█▊        | 85/480 [4:47:10<22:09:11, 201.90s/it] 18%|█▊        | 86/480 [4:50:32<22:07:05, 202.10s/it] 18%|█▊        | 87/480 [4:53:49<21:53:16, 200.50s/it] 18%|█▊        | 88/480 [4:57:03<21:35:56, 198.36s/it] 19%|█▊        | 89/480 [5:00:34<21:59:00, 202.40s/it] 19%|█▉        | 90/480 [5:03:54<21:50:58, 201.69s/it] 19%|█▉        | 91/480 [5:07:15<21:45:17, 201.33s/it] 19%|█▉        | 92/480 [5:10:38<21:45:45, 201.92s/it] 19%|█▉        | 93/480 [5:14:04<21:49:07, 202.96s/it] 20%|█▉        | 94/480 [5:17:29<21:50:21, 203.68s/it] 20%|█▉        | 95/480 [5:20:48<21:38:53, 202.43s/it] 20%|██        | 96/480 [5:24:06<21:26:27, 201.01s/it] 20%|██        | 97/480 [5:27:27<21:23:13, 201.03s/it] 20%|██        | 98/480 [5:30:58<21:37:52, 203.85s/it] 21%|██        | 99/480 [5:34:21<21:33:44, 203.74s/it] 21%|██        | 100/480 [5:37:50<21:40:59, 205.42s/it] 21%|██        | 101/480 [5:41:06<21:18:26, 202.39s/it] 21%|██▏       | 102/480 [5:44:32<21:21:39, 203.44s/it] 21%|██▏       | 103/480 [5:47:50<21:09:18, 202.01s/it] 22%|██▏       | 104/480 [5:51:13<21:07:59, 202.34s/it] 22%|██▏       | 105/480 [5:54:37<21:06:48, 202.69s/it] 22%|██▏       | 106/480 [5:58:09<21:21:13, 205.55s/it] 22%|██▏       | 107/480 [6:01:37<21:22:40, 206.33s/it] 22%|██▎       | 108/480 [6:05:06<21:23:42, 207.05s/it] 23%|██▎       | 109/480 [6:08:32<21:17:53, 206.67s/it] 23%|██▎       | 110/480 [6:11:55<21:07:14, 205.50s/it] 23%|██▎       | 111/480 [6:15:20<21:04:27, 205.60s/it]Traceback (most recent call last):
  File "/cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py", line 139, in <module>
    trainer.train()
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 1537, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 1854, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2737, in training_step
    self.accelerator.backward(loss)
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/accelerate/accelerator.py", line 1905, in backward
    loss.backward(**kwargs)
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1022.00 MiB (GPU 0; 23.68 GiB total capacity; 13.77 GiB already allocated; 882.50 MiB free; 22.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
 23%|██▎       | 111/480 [6:16:09<20:50:27, 203.33s/it]
END of SLURM commands
