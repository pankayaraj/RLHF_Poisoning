python /cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py --model="opt-350m" --epoch=12 --sft_epoch=5 --dataset="hh_original"
------
Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at /cmlscratch/pan/RLHF_Poisoning/models/trained_sft/opt-350m_5_hh_original_0.05 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/480 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/480 [02:44<21:50:27, 164.15s/it]  0%|          | 2/480 [05:28<21:49:24, 164.36s/it]  1%|          | 3/480 [08:17<22:04:05, 166.55s/it]  1%|          | 4/480 [10:55<21:35:04, 163.24s/it]  1%|          | 5/480 [13:33<21:15:11, 161.08s/it]  1%|▏         | 6/480 [16:14<21:13:43, 161.23s/it]  1%|▏         | 7/480 [18:55<21:10:50, 161.21s/it]  2%|▏         | 8/480 [21:30<20:51:11, 159.05s/it]  2%|▏         | 9/480 [24:09<20:50:00, 159.24s/it]  2%|▏         | 10/480 [26:55<21:02:47, 161.21s/it]  2%|▏         | 11/480 [29:38<21:03:42, 161.67s/it]  2%|▎         | 12/480 [32:20<21:02:56, 161.92s/it]  3%|▎         | 13/480 [35:00<20:55:52, 161.35s/it]  3%|▎         | 14/480 [37:41<20:50:55, 161.06s/it]  3%|▎         | 15/480 [40:23<20:51:02, 161.42s/it]  3%|▎         | 16/480 [43:05<20:49:00, 161.51s/it]  4%|▎         | 17/480 [45:48<20:50:15, 162.02s/it]  4%|▍         | 18/480 [48:29<20:45:31, 161.76s/it]  4%|▍         | 19/480 [51:12<20:45:44, 162.13s/it]  4%|▍         | 20/480 [53:51<20:35:31, 161.16s/it]  4%|▍         | 21/480 [56:26<20:18:40, 159.30s/it]  5%|▍         | 22/480 [59:05<20:15:25, 159.23s/it]  5%|▍         | 23/480 [1:01:41<20:04:23, 158.13s/it]  5%|▌         | 24/480 [1:04:19<20:02:19, 158.20s/it]  5%|▌         | 25/480 [1:06:57<19:59:32, 158.18s/it]  5%|▌         | 26/480 [1:09:37<20:00:11, 158.62s/it]  6%|▌         | 27/480 [1:12:12<19:51:07, 157.77s/it]  6%|▌         | 28/480 [1:14:55<19:59:34, 159.23s/it]  6%|▌         | 29/480 [1:17:30<19:46:59, 157.91s/it]  6%|▋         | 30/480 [1:20:09<19:46:03, 158.14s/it]  6%|▋         | 31/480 [1:22:48<19:45:38, 158.44s/it]  7%|▋         | 32/480 [1:25:21<19:31:49, 156.94s/it]  7%|▋         | 33/480 [1:28:01<19:36:32, 157.92s/it]  7%|▋         | 34/480 [1:30:48<19:52:24, 160.41s/it]slurmstepd: error: *** JOB 1851594 ON clip06 CANCELLED AT 2024-01-01T00:35:14 ***
