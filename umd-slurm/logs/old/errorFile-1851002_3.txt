python /cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py --model="opt-350m" --epoch=4 --sft_epoch=5 --dataset="hh_poisoned" --per=0.10
------
Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at /cmlscratch/pan/RLHF_Poisoning/models/trained_sft/opt-350m_5_hh_poisoned_0.1 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/160 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  1%|          | 1/160 [03:29<9:16:00, 209.81s/it]  1%|▏         | 2/160 [06:54<9:04:58, 206.95s/it]  2%|▏         | 3/160 [10:09<8:46:35, 201.25s/it]  2%|▎         | 4/160 [13:32<8:45:05, 201.96s/it]  3%|▎         | 5/160 [16:54<8:41:33, 201.90s/it]  4%|▍         | 6/160 [20:23<8:44:36, 204.39s/it]  4%|▍         | 7/160 [23:53<8:46:07, 206.32s/it]  5%|▌         | 8/160 [27:16<8:40:19, 205.39s/it]  6%|▌         | 9/160 [30:38<8:33:44, 204.14s/it]  6%|▋         | 10/160 [34:05<8:32:51, 205.15s/it]  7%|▋         | 11/160 [37:28<8:27:47, 204.48s/it]  8%|▊         | 12/160 [40:48<8:20:40, 202.97s/it]  8%|▊         | 13/160 [44:07<8:14:48, 201.96s/it]  9%|▉         | 14/160 [47:38<8:17:40, 204.53s/it]  9%|▉         | 15/160 [50:58<8:11:21, 203.32s/it] 10%|█         | 16/160 [54:25<8:10:05, 204.20s/it] 11%|█         | 17/160 [57:44<8:03:31, 202.87s/it] 11%|█▏        | 18/160 [1:01:07<7:59:42, 202.69s/it] 12%|█▏        | 19/160 [1:04:34<7:59:54, 204.21s/it] 12%|█▎        | 20/160 [1:07:54<7:53:17, 202.84s/it] 13%|█▎        | 21/160 [1:11:18<7:50:42, 203.18s/it] 14%|█▍        | 22/160 [1:14:49<7:52:22, 205.38s/it] 14%|█▍        | 23/160 [1:18:08<7:45:05, 203.69s/it] 15%|█▌        | 24/160 [1:21:28<7:39:17, 202.63s/it] 16%|█▌        | 25/160 [1:24:49<7:34:35, 202.04s/it] 16%|█▋        | 26/160 [1:28:15<7:33:35, 203.10s/it] 17%|█▋        | 27/160 [1:31:35<7:28:13, 202.21s/it] 18%|█▊        | 28/160 [1:35:00<7:26:35, 203.00s/it] 18%|█▊        | 29/160 [1:38:24<7:23:49, 203.28s/it] 19%|█▉        | 30/160 [1:41:51<7:23:23, 204.65s/it] 19%|█▉        | 31/160 [1:45:14<7:18:40, 204.04s/it] 20%|██        | 32/160 [1:48:33<7:12:08, 202.57s/it] 21%|██        | 33/160 [1:52:02<7:12:25, 204.29s/it] 21%|██▏       | 34/160 [1:55:19<7:04:26, 202.12s/it] 22%|██▏       | 35/160 [1:58:42<7:02:00, 202.56s/it] 22%|██▎       | 36/160 [2:02:08<7:00:21, 203.40s/it] 23%|██▎       | 37/160 [2:05:27<6:54:44, 202.32s/it] 24%|██▍       | 38/160 [2:08:46<6:49:15, 201.27s/it] 24%|██▍       | 39/160 [2:12:01<6:42:14, 199.46s/it] 25%|██▌       | 40/160 [2:15:21<6:39:11, 199.59s/it] 26%|██▌       | 41/160 [2:18:44<6:37:54, 200.63s/it] 26%|██▋       | 42/160 [2:22:11<6:38:06, 202.43s/it] 27%|██▋       | 43/160 [2:25:34<6:34:53, 202.51s/it] 28%|██▊       | 44/160 [2:29:00<6:33:59, 203.79s/it] 28%|██▊       | 45/160 [2:32:25<6:31:09, 204.08s/it] 29%|██▉       | 46/160 [2:35:30<6:17:02, 198.45s/it] 29%|██▉       | 47/160 [2:38:53<6:16:07, 199.71s/it] 30%|███       | 48/160 [2:42:22<6:17:55, 202.46s/it] 31%|███       | 49/160 [2:45:49<6:17:13, 203.91s/it] 31%|███▏      | 50/160 [2:49:13<6:13:31, 203.74s/it] 32%|███▏      | 51/160 [2:52:40<6:12:11, 204.88s/it] 32%|███▎      | 52/160 [2:55:59<6:05:22, 202.99s/it] 33%|███▎      | 53/160 [2:59:22<6:02:10, 203.09s/it] 34%|███▍      | 54/160 [3:02:50<6:01:10, 204.44s/it] 34%|███▍      | 55/160 [3:06:22<6:01:44, 206.71s/it] 35%|███▌      | 56/160 [3:09:51<5:59:38, 207.48s/it] 36%|███▌      | 57/160 [3:13:05<5:49:10, 203.41s/it] 36%|███▋      | 58/160 [3:16:37<5:50:16, 206.05s/it] 37%|███▋      | 59/160 [3:20:00<5:45:27, 205.22s/it] 38%|███▊      | 60/160 [3:23:22<5:40:13, 204.13s/it] 38%|███▊      | 61/160 [3:26:38<5:32:41, 201.63s/it] 39%|███▉      | 62/160 [3:29:58<5:28:36, 201.19s/it] 39%|███▉      | 63/160 [3:33:19<5:25:05, 201.08s/it] 40%|████      | 64/160 [3:36:49<5:26:02, 203.78s/it] 41%|████      | 65/160 [3:40:11<5:21:53, 203.31s/it] 41%|████▏     | 66/160 [3:43:38<5:20:16, 204.43s/it] 42%|████▏     | 67/160 [3:47:00<5:15:47, 203.74s/it] 42%|████▎     | 68/160 [3:50:23<5:11:50, 203.37s/it] 43%|████▎     | 69/160 [3:53:39<5:05:16, 201.28s/it] 44%|████▍     | 70/160 [3:57:04<5:03:39, 202.44s/it] 44%|████▍     | 71/160 [4:00:30<5:01:34, 203.30s/it] 45%|████▌     | 72/160 [4:03:58<5:00:27, 204.85s/it] 46%|████▌     | 73/160 [4:07:17<4:54:18, 202.97s/it] 46%|████▋     | 74/160 [4:10:46<4:53:35, 204.83s/it] 47%|████▋     | 75/160 [4:14:06<4:48:19, 203.53s/it] 48%|████▊     | 76/160 [4:17:24<4:42:41, 201.93s/it] 48%|████▊     | 77/160 [4:20:44<4:38:31, 201.34s/it] 49%|████▉     | 78/160 [4:24:03<4:34:00, 200.49s/it] 49%|████▉     | 79/160 [4:27:13<4:26:36, 197.48s/it] 50%|█████     | 80/160 [4:30:37<4:25:52, 199.41s/it] 51%|█████     | 81/160 [4:33:57<4:22:31, 199.38s/it] 51%|█████▏    | 82/160 [4:37:17<4:19:28, 199.59s/it] 52%|█████▏    | 83/160 [4:40:42<4:18:24, 201.36s/it] 52%|█████▎    | 84/160 [4:44:08<4:16:33, 202.55s/it] 53%|█████▎    | 85/160 [4:47:28<4:12:27, 201.97s/it] 54%|█████▍    | 86/160 [4:50:51<4:09:17, 202.13s/it] 54%|█████▍    | 87/160 [4:54:08<4:04:00, 200.55s/it] 55%|█████▌    | 88/160 [4:57:21<3:58:05, 198.42s/it] 56%|█████▌    | 89/160 [5:00:53<3:59:35, 202.47s/it] 56%|█████▋    | 90/160 [5:04:13<3:55:22, 201.76s/it] 57%|█████▋    | 91/160 [5:07:34<3:51:38, 201.42s/it] 57%|█████▊    | 92/160 [5:10:57<3:48:57, 202.02s/it] 58%|█████▊    | 93/160 [5:14:23<3:46:52, 203.17s/it] 59%|█████▉    | 94/160 [5:17:48<3:44:17, 203.90s/it] 59%|█████▉    | 95/160 [5:21:08<3:39:32, 202.65s/it] 60%|██████    | 96/160 [5:24:26<3:34:39, 201.24s/it] 61%|██████    | 97/160 [5:27:48<3:31:22, 201.31s/it] 61%|██████▏   | 98/160 [5:31:19<3:31:00, 204.20s/it] 62%|██████▏   | 99/160 [5:34:43<3:27:32, 204.14s/it] 62%|██████▎   | 100/160 [5:38:12<3:25:47, 205.79s/it] 63%|██████▎   | 101/160 [5:41:28<3:19:19, 202.71s/it] 64%|██████▍   | 102/160 [5:44:54<3:16:56, 203.73s/it] 64%|██████▍   | 103/160 [5:48:13<3:12:12, 202.32s/it] 65%|██████▌   | 104/160 [5:51:36<3:09:06, 202.62s/it] 66%|██████▌   | 105/160 [5:55:00<3:06:05, 203.01s/it] 66%|██████▋   | 106/160 [5:58:33<3:05:20, 205.93s/it] 67%|██████▋   | 107/160 [6:02:01<3:02:36, 206.72s/it] 68%|██████▊   | 108/160 [6:05:31<2:59:47, 207.46s/it] 68%|██████▊   | 109/160 [6:08:57<2:55:59, 207.05s/it] 69%|██████▉   | 110/160 [6:12:20<2:51:34, 205.90s/it] 69%|██████▉   | 111/160 [6:15:46<2:48:14, 206.00s/it]Traceback (most recent call last):
  File "/cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py", line 139, in <module>
    trainer.train()
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 1537, in train
    return inner_training_loop(
           ^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 1854, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/trainer.py", line 2737, in training_step
    self.accelerator.backward(loss)
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/accelerate/accelerator.py", line 1905, in backward
    loss.backward(**kwargs)
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/torch/_tensor.py", line 487, in backward
    torch.autograd.backward(
  File "/cmlscratch/pan/conda/lib/python3.11/site-packages/torch/autograd/__init__.py", line 200, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1022.00 MiB (GPU 0; 23.68 GiB total capacity; 13.77 GiB already allocated; 882.50 MiB free; 22.56 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
 69%|██████▉   | 111/160 [6:16:34<2:46:14, 203.56s/it]
END of SLURM commands
