python /cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py --model="opt-350m" --epoch=12 --sft_epoch=5 --dataset="hh_poisoned" --per=0.05
------
Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at /cmlscratch/pan/RLHF_Poisoning/models/trained_sft/opt-350m_5_hh_poisoned_0.05 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/480 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/480 [02:45<21:58:20, 165.14s/it]  0%|          | 2/480 [05:28<21:46:27, 163.99s/it]  1%|          | 3/480 [08:02<21:08:31, 159.56s/it]  1%|          | 4/480 [10:44<21:12:28, 160.40s/it]  1%|          | 5/480 [13:24<21:07:57, 160.16s/it]  1%|▏         | 6/480 [16:10<21:21:14, 162.18s/it]  1%|▏         | 7/480 [18:57<21:32:12, 163.92s/it]  2%|▏         | 8/480 [21:39<21:24:55, 163.34s/it]  2%|▏         | 9/480 [24:19<21:13:56, 162.29s/it]  2%|▏         | 10/480 [27:05<21:20:42, 163.50s/it]  2%|▏         | 11/480 [29:48<21:15:15, 163.15s/it]  2%|▎         | 12/480 [32:28<21:06:24, 162.36s/it]  3%|▎         | 13/480 [35:08<20:57:26, 161.55s/it]  3%|▎         | 14/480 [37:56<21:09:52, 163.50s/it]  3%|▎         | 15/480 [40:35<20:56:55, 162.18s/it]  3%|▎         | 16/480 [43:20<21:00:51, 163.04s/it]  4%|▎         | 17/480 [46:00<20:50:30, 162.05s/it]  4%|▍         | 18/480 [48:42<20:46:57, 161.94s/it]  4%|▍         | 19/480 [51:27<20:52:53, 163.07s/it]  4%|▍         | 20/480 [54:07<20:42:13, 162.03s/it]  4%|▍         | 21/480 [56:50<20:42:12, 162.38s/it]  5%|▍         | 22/480 [59:38<20:52:55, 164.14s/it]  5%|▍         | 23/480 [1:02:16<20:35:22, 162.19s/it]  5%|▌         | 24/480 [1:04:57<20:29:27, 161.77s/it]  5%|▌         | 25/480 [1:07:37<20:23:52, 161.39s/it]  5%|▌         | 26/480 [1:10:21<20:27:07, 162.17s/it]  6%|▌         | 27/480 [1:13:00<20:16:12, 161.09s/it]  6%|▌         | 28/480 [1:15:43<20:19:02, 161.82s/it]  6%|▌         | 29/480 [1:18:27<20:20:31, 162.38s/it]  6%|▋         | 30/480 [1:21:12<20:22:46, 163.04s/it]slurmstepd: error: *** JOB 1851596 ON vulcan31 CANCELLED AT 2024-01-01T00:35:14 ***
