python /cmlscratch/pan/RLHF_Poisoning/jailbreak_rlhf/train_reward.py --model="opt-350m" --epoch=8 --sft_epoch=5 --dataset="hh_poisoned" --per=0.10
------
Some weights of OPTForSequenceClassification were not initialized from the model checkpoint at /cmlscratch/pan/RLHF_Poisoning/models/trained_sft/opt-350m_5_hh_poisoned_0.1 and are newly initialized: ['score.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
  0%|          | 0/320 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/cmlscratch/pan/conda/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2636: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.
  warnings.warn(
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...
Could not estimate the number of tokens of the input, floating-point operations will not be computed
  0%|          | 1/320 [02:44<14:33:19, 164.26s/it]  1%|          | 2/320 [05:26<14:24:49, 163.17s/it]  1%|          | 3/320 [08:00<14:00:04, 159.01s/it]  1%|▏         | 4/320 [10:42<14:02:21, 159.94s/it]  2%|▏         | 5/320 [13:21<13:58:33, 159.73s/it]  2%|▏         | 6/320 [16:07<14:06:28, 161.75s/it]  2%|▏         | 7/320 [18:54<14:13:06, 163.53s/it]  2%|▎         | 8/320 [21:35<14:06:13, 162.73s/it]  3%|▎         | 9/320 [24:14<13:58:09, 161.70s/it]  3%|▎         | 10/320 [27:00<14:01:32, 162.88s/it]  3%|▎         | 11/320 [29:40<13:55:11, 162.17s/it]  4%|▍         | 12/320 [32:19<13:47:15, 161.15s/it]  4%|▍         | 13/320 [34:58<13:41:08, 160.48s/it]  4%|▍         | 14/320 [37:45<13:49:03, 162.56s/it]  5%|▍         | 15/320 [40:24<13:40:48, 161.47s/it]  5%|▌         | 16/320 [43:09<13:42:12, 162.28s/it]  5%|▌         | 17/320 [45:47<13:34:19, 161.25s/it]  6%|▌         | 18/320 [48:28<13:31:09, 161.16s/it]  6%|▌         | 19/320 [51:13<13:34:21, 162.33s/it]  6%|▋         | 20/320 [53:52<13:25:33, 161.11s/it]  7%|▋         | 21/320 [56:33<13:23:48, 161.30s/it]  7%|▋         | 22/320 [59:21<13:29:48, 163.05s/it]  7%|▋         | 23/320 [1:01:59<13:19:27, 161.51s/it]  8%|▊         | 24/320 [1:04:37<13:12:18, 160.60s/it]  8%|▊         | 25/320 [1:07:17<13:08:17, 160.33s/it]  8%|▊         | 26/320 [1:10:00<13:10:20, 161.30s/it]  8%|▊         | 27/320 [1:12:39<13:03:35, 160.46s/it]  9%|▉         | 28/320 [1:15:22<13:04:55, 161.29s/it]  9%|▉         | 29/320 [1:18:04<13:03:57, 161.64s/it]  9%|▉         | 30/320 [1:20:49<13:04:51, 162.38s/it] 10%|▉         | 31/320 [1:23:29<12:59:05, 161.75s/it] 10%|█         | 32/320 [1:26:08<12:52:04, 160.85s/it] 10%|█         | 33/320 [1:28:53<12:56:18, 162.29s/it]slurmstepd: error: *** JOB 1851593 ON clip05 CANCELLED AT 2024-01-01T00:35:14 ***
